var tipuesearch = {"pages": [{'title': 'About', 'text': '本課程為協同產品設計實習, 每週上課時數為 4 小時, 其中兩小時為教學時段, 另兩小時為實習時段, 課程內容包含 Programming 與 Computer aided Design 等兩大部分. \n Programming 部分包含計算機程式與網際內容管理等課程的內容: Brython, Python, Lua 與 C/C++ 等程式的編寫與 Github 整合應用. \n Computer aided Design 部分包含電腦輔助設計與實習課程的內容: Solvespace (C++), Siemens Solid Edge (Python), Siemens NX (Python), Onshape (Featurescript), CoppeliaSim (Lua, Python, C++) 與 Webots (Python, C++) 等套件的應用. \n 課程評分: 線上測驗 (50%), 出席、協同專案與自我評量 (50%). \n 倉儲:  https://github.com/mdecycu/cd2023 \n 網站:  https://mde.tw/cd2023 \n MS Teams: 2q56a7l (for @nfu users only) \n 本課程將採兩人一組、四人一組與八人一組的方式進行協同機電整合產品開發. \n 行事曆 \n 全頁檢視 \n \n', 'tags': '', 'url': 'About.html'}, {'title': 'Motivation', 'text': '學習協同產品設計與實習課程的動機: \n 了解如何在 Web-based 多人協同環境中執行產品開發 \n 課程目標: \n 各分組開發一款能在 web-based CoppeliaSim 場景中, 雙方或多方 (human or computer) 對玩的遊戲 (game) 產品. \n CoppeliaSim scene on Browser \n Human to human game \n Human to computer game \n Computer to Computer game \n Get started:  https://mde.tw/cd2023/content/CoppeliaSim.html \n \n 產品設計 (design) 與產品開發 (development) 有何區別？ \n https://uk.indeed.com/career-advice/career-development/design-vs-development \n 教導或學習協同產品設計實習的過程, 必須自問學這東西有用嗎? 您自己 (平時, 一直) 使用嗎? \n 讓我們回到 2012 年, review 一下 Collaborative Product Development 有關的文獻: \n 2012 - Collaborative Product Development - a literature overview  ( in png  for @nfu users only) \n In a market where competitive pressures surround the industry, organisations face the demand for customised solutions (客製化解題方案), high-quality products, short time-to-market and lower costs. \n To ensure survival and continued prosperity, businesses must meet these challenges by providing a constant stream (源源不斷) of new and improved products, processes and services. Product Development and innovation emerges consequently as a key process of competition and sustainability (永續). \n Inter-firm collaborations emerge in order to share product development risks (風險) and costs as well as shorten time-to-market and obtain additional benefits, such as reducing research and development (R&D, 研發) costs, increasing market share and exchanging expertise. \n Collaborative product development, in brief, can be defined as a collaborative process overlapping (重疊) with the product development process. \n The existing literature is analysed in three groups (CPD dynamics, partnership formation (夥伴關係組成) in CPD and CPD infrastructure) \n CPD is "any activity where two (or more) partners contribute differential (不同) resources and know how to agree complementary (互補) aims in order to design and develop a new or improved product" \n By combining the strength and expertise of the best diverse and geographically dispersed product development teams, better mission scenarios, designs and the corresponding products and technologies can be developed in less time. \n Applications of Collaborative Product Development include  networks of companies  (拓展公司間成員關係與連結的組織,  business network ),  virtual organisations ,  customer–supplier collaboration ,  extended (manufacturing) enterprises  (延伸企業),  dynamic networks ,  strategic alliances  (策略聯盟)and\xa0  joint ventures  (合資企業). \n 同時也看看  Why you should pursue collaborative design to build products  以及  CPD 介紹 . \n Collaborative design involves bringing together a diverse group of people from different backgrounds and industries to work together and generate new ideas. By leveraging (運用) this diverse group’s (多元群組間) unique perspectives and expertise, creating truly unique and effective solutions is possible. \n An architect (架構師) will think of the software architecture, a designer (設計者) will have an idea of how to make the product usable (能想出方法讓產品可用), the developer (開發者) will know whether the proposed design is feasible (知道提出的設計是否可行), and the stakeholders (股東) will have opinions around the product design to make it invest worthy (確定該產品設計值得投資). Overall, the experience design process is a collaborative initiative undertaken by a product development company, not a one-person or one-team job. \n Collaborative design typically involves a series of meetings and workshops where stakeholders (利益相關者) can share their ideas and discuss potential solutions to the problems being addressed by the product. This approach ensures that the final product is well-designed, effective, and meets the needs of all stakeholders. \n 2016 A Framework of Cloud-based Collaborative Platform to Integrate Product Design Requests and Contradiction Analysis  (for @nfu users only) \n 2017 A Strategic Planning of TRIZ Applications for Cloud-based Collaborative Product Development  (for @nfu users only) \n 2023 Beginner’s Guide to Streamlit with Python  (for @nfu users only) \n https://streamlit.io \n 2023 Productionizing AI  (for @nfu users only) \n 2023 Industrial Robotics Control Mathematical Models, Software Architecture, and Electronics Design  (for @nfu users only) \n 2023 Computational Mechanics with Deep Learning An Introduction  (for @nfu users only) \n 2001 Introduction to Physical Modeling with Modelica  (for @nfu users only) \n Modelica model for a youbot manipulator \n OpenModelica User Guide \n The OpenModelica Integrated Modeling, Simulation and Optimization Environment \n \n', 'tags': '', 'url': 'Motivation.html'}, {'title': 'Reading', 'text': '1999 How to be a star engineer ,  如何成為一位傑出的工程師  (for @nfu users only) \n 2012 - Collaborative Product Development - a literature overview  ( in png  for @nfu users only) \n 這是一篇長達 20 頁, 有關協同產品開發的文獻探討與概述, 其中牽涉許多與產品開發流程及國際大環境下企業競爭常見的專業用詞 (Terminology), 非常適合機械設計工程師仔細閱讀. 從中可以了解在目前機電資 (Infomechatronic) 與虛實整合 (Cyber-Physical Integration) 產品開發模式下, 每一個參與協同設計的成員或團隊, 除了必須專注在自己的專業領域外, 對於時間與機會成本的掌控, 採行方案的永續性, 理念與創意的展示方法, 以及對於跨領域知識的涉獵, 都是未來進入產業之前就必須要在心態與身體力行中, 不斷調整歷練與實踐. \n Why you should pursue collaborative design to build products  以及  CPD 介紹 . \n 在軟體開發界, 經常會有所謂研究者, 架構師, 開發者與設計者等不同專業任務分工, 其中研究者負責廣泛涉獵各種技術的應用, 架構師則主導公司產品開發的大方向與所採行的技術細節, 開發者則承接架構師所賦予的產品開發任務, 設法找出可行的產品開發方案, 其中包括規劃及管理與產品開發有關的市場導向, 期程與成本規劃等專案細節, 以便讓設計者根據專案計畫中的細項, 解決各種產品開發所面臨的問題. \n 2016 A Framework of Cloud-based Collaborative Platform to Integrate Product Design Requests and Contradiction Analysis  (for @nfu users only) \n 2017 A Strategic Planning of TRIZ Applications for Cloud-based Collaborative Product Development  (for @nfu users only) \n 2023 Beginner’s Guide to Streamlit with Python  (for @nfu users only) \n 2023 Industrial Robotics Control Mathematical Models, Software Architecture, and Electronics Design  (for @nfu users only) \n 2023 Computational Mechanics with Deep Learning An Introduction  (for @nfu users only) \n 2001 Introduction to Physical Modeling with Modelica  (for @nfu users only) \n \n', 'tags': '', 'url': 'Reading.html'}, {'title': 'Projects', 'text': '二甲: w2 and w7 調整放假. \n w2 (2/27, 星期一) 課程將在 3/9 (星期四) 第九至第十二堂補課 (17:20 - 20:55) \n w7 (4/3, 星期一) 課程將在 3/30 (星期四) 第九至第十二堂補課 (17:20 - 20:55) \n 二乙: w7 and w18 放假 \n 主題:  開發一款能在 web-based CoppeliaSim 場景中雙方或多方 (human or computer) 對玩的遊戲 (game) 產品 . \n w1-w5: 兩人一組, 根據自選產品在期限內完成產品開發, 以抽點方式在 w4 現場發表兩人協同四週後所完成的產品, 在 w5 各組採 OBS + Teams 以影片發表所完成的協同產品. ( 參考資料 ) \n 請各組設法直接利用網路工具, 分別對各組互相評分, 列出評分項目與具體依據後, 依照各組總評分排序. (該如何評分？如何排序? 如何對所有成員給定 Project 1 得分？) \n w6-w10: 四人一組, 根據自選產品在期限內完成產品開發, 以抽點方式在 w10 現場發表兩人協同四週後所完成的產品, 在 w11 各組採 OBS + Teams 以影片發表所完成的協同產品. ( 參考資料 ) \n 請各組設法直接利用網路工具, 分別對各組互相評分, 列出評分項目與具體依據後, 依照各組總評分排序. (該如何評分？如何排序? 如何對所有成員給定 Project 2 得分？) \n w11-w17: 八人一組, 根據自選產品在期限內完成產品開發, 以抽點方式在 w16 現場發表兩人協同四週後所完成的產品, 在 w17 各組採 OBS + Teams 以影片發表所完成的協同產品. ( 參考資料 ) \n 請各組設法直接利用網路工具, 分別對各組互相評分, 列出評分項目與具體依據後, 依照各組總評分排序. (該如何評分？如何排序? 如何對所有成員給定 Project 3 得分？) \n Test modules (考試題庫模組): 於 w4, w11 與 w16 各組必須針對協同專案執行過程所需了解的議題, 製作符合 TCExam 格式的考試題目, 每人每一階段至少提供 5 個多選題題目, 每一題各有至少 5 個正確敘述與 5 個錯誤敘述. \n TCExam tests: 於 w5, w12 與 w17 舉行. \n Reference: \n \n FoosBall (手足球):  https://youtu.be/ZRW793-cZtc \n Ping Pong (乒乓球):  https://youtu.be/l_tzXQC4His \n Football (足球):  https://youtu.be/cYgDqiKRJ_U ,  https://youtu.be/oFV6NyITuhw \n Baseball (棒球):  https://youtu.be/KqPlggkBzXI ,  https://www.youtube.com/shorts/4rk8KrtAkzQ \n Curling (冰壺遊戲):  https://youtu.be/DkYUh5X0ZvM \n Ball Game \n Game \n Configuring Joints:  https://youtu.be/24eVkpFo_xE \n Joint Types:  https://youtu.be/akr7mU-1qoc \n Programming Joints:  https://youtu.be/YFpXZN3EKfY \n Joints:  https://youtu.be/YcfARpQVKhU ,  https://youtu.be/rbwWMYwL3rI \n \n \n', 'tags': '', 'url': 'Projects.html'}, {'title': 'pj1', 'text': 'resume:  https://github.com/mdecd2023/resume-scrum-1 \n 2a-pj1agx:\xa0 https://github.com/mdecd2023/2a-pj1agx \n cmsimde:  https://github.com/mdecycu/cmsimde \n', 'tags': '', 'url': 'pj1.html'}, {'title': 'Stud2', 'text': 'stud2 是一台 Ubuntu server, 讓每一個用戶透過兩個 ports 共享 server\xa0 中所設置的外部與內部 ports. \n 其中以 9 開頭的 port 設定為內部 port,\xa0 而 8 開頭的 port 則用於外部連線. \n 當某一用戶誤用所分配的內部 port 時, 管理者就可以透過 lsof 指令找出該用戶, 並即時加以糾正. \n 又由於 8 開頭的外部 port 統一由管理者以 Stunnel 啟動並加以管控, 因此一般用戶產生誤用的情況只會發生在內部以 9 開頭的 port. \n 列出 Ubuntu 中 port 使用相關資訊: lsof -i :9123 \n lsof: list open files \n -i: selects the listing of files any of whose Internet address matches the address specified in i.\xa0 If no address is specified, this option selects the listing of all Internet and x.25 (HP-UX) network files. \n', 'tags': '', 'url': 'Stud2.html'}, {'title': 'Web-based', 'text': 'Front-end 使用瀏覽器, Back-end 使用 WWW server 進行機電資協同產品開發 \n Creating Web-based Laboratory  (2004) \n Web-based Control and Robotics Education  (2009) \n Learning Web-based Virtual Reality  (2017) \n', 'tags': '', 'url': 'Web-based.html'}, {'title': 'Blender', 'text': 'https://www.blender.org/ \n Blender history:  https://www.blender.org/about/history/ \n Blender on Github:  https://github.com/blender \n Ebooks on Springer: \n The Blender Python API  (2017) \n Beginning Blender  (2010) \n Core Blender Development  (2021) \n Modeling and Animation using Blender  (2020) \n Creating Game Environment in Blender 3D  (2020) \n Introduction to Blender 3.0  (2022) \n Foundation Blender Compositing  (2009) \n 機械設計工程師能拿 Blender 做甚麼? \n https://workwut.com/mechanical-engineers-blender/ \n https://blender.stackexchange.com/questions/53293/is-blender-actually-useable-for-engineering \n https://youtu.be/AD_jyBN09jA \n https://blendergrid.com/learn/articles/tyler-disney-interview \n', 'tags': '', 'url': 'Blender.html'}, {'title': 'TCExam', 'text': "設定是否讓用戶自行註冊帳號: \n tcexam\\shared\\config\\tce_user_registration.php \n \xa0/* kmol1*/ define('K_USRREG_ENABLED', false); \n", 'tags': '', 'url': 'TCExam.html'}, {'title': '2a 註冊', 'text': 'tier \n 41023132 41023154 41023113 41023119 41023125 41023138 41039138 41023108 41023146 41023120 41023124 41023114', 'tags': '', 'url': '2a 註冊.html'}, {'title': 'Ref', 'text': '2014 \n https://github.com/coursemdetw?tab=repositories \n 2014 cp:  https://vimeo.com/115314619 \n https://vimeo.com/user24079973 \n 2015 \n https://github.com/2015fallhw?tab=repositories \n https://github.com/2015fallhw/cdw2 \n https://2015cdg11.wordpress.com/ \n 2017 \n https://40423245.github.io/2017springcd_hw/blog/pages/about/ \n https://www.youtube.com/channel/UCJ0Ye3mhMApH0yxAcwCXs5g/videos \n 2018 \n https://github.com/mdecadp2018 \n https://scrum-1.gitbooks.io/cd2018/content/cd.html \n https://www.ithome.com.tw/news/125308 \n 2019 \n https://github.com/mdekmol \n https://github.com/scrum-1/cd2019a \xa0 \n 2020 \n https://gitter.im/mdecourse/cp2020 \n https://gitter.im/mdecourse/cad2020 \xa0 \n https://gitter.im/mdecourse/cd2020 \n 2021 \n https://gitter.im/mdecourse/cp2021 \n https://gitter.im/mdecourse/cad2021 \n https://gitter.im/mdecourse/cd2021 \n https://www.youtube.com/@user-yb6kb6je7f/videos \n https://www.youtube.com/@user-yb6kb6je7f/playlists \n 2022: \n http://wcm.cycu.org:88/github/cd2022 \xa0(for @nfu users only) \n http://wcm.cycu.org:88/github/cd2022_guide/ \xa0(for @nfu users only) \n https://gitter.im/mdecourse/cd2022 \xa0 \n cd2022_w13_2a_part1.mp4 \xa0(18:17 有關電腦的規劃與相關套件) \n cd2022_w13_2a_part2.mp4  (33:37  uArm  機械手臂 NX 模型介紹) \n cd2022_w13_2a_part3.mp4  (28:21  Leo Editor  (含 importer) 在 parse xml 的使用介紹) \n 節點標題指令: @auto, @clean, @edit \n CoppeliaSim parse STL 組立件檔案與座標轉換與縮放 \n CoppeliaSim Python scripting \n cd2022_w13_2b_part1.mp4  (24:05 NX and CoppeliaSim 應用說明) \n cd2022_w13_2b_part2.mp4  (18:27\xa0 uArm \xa0 機械手臂 NX 模型介紹) \n cd2022_w13_2b_part3.mp4  (7:47 利用 Brython 擷取建立學員網站連結) \n cd2022_w14_2a_part1.mp4  (19:33 CoppeliaSim 4.3.0 Python Scripting 與場景介紹) \n cd2022_w14_2a_part2.mp4  (32:54 NX 零組件轉入 CoppeliaSim 建立 MTB robot 場景介紹) \n cd2022_w14_2a_part3.mp4  (26:27 uArm 機械手臂參數化零組件設計繪圖與網站解決衝突) \n cd2022_w14_2b_part1.mp4  (23:14 CoppeliaSim Lua 與 Python Scripting 介紹) \n cd2022_w14_2b_part2.mp4  (33:08 建立 CoppeliaSim MTB robot 場景介紹) \n cd2022_w14_2b_part3.mp4  (34:45 MTB robot 場景與 scripting 程式介紹) \n cd2022_w15_2a.mp4  (55:30 Mac 執行 CoppeliaSim, 協同設計考量, Ruckig.com 介紹) \n cd2022_w15_2b.mp4  (31:25 uArm robot 場景介紹, IK 運算, Feedback 與多 port 控制) \n planar_2_link_robot_IK_part1.mp4  (5:24 for @nfu users only) \n planar_2_link_robot_IK_part2.mp4  (3:03 for @nfu users only) \n cd2022_w16_2a_part1.mp4  (40:22 tkinter GUI robot 控制, CoppeliaSim remote API 介紹) \n cd2022_w16_2a_part2.mp4  (21:18 remote API 控制與 Image sensor 串流) \n Planar 3R IK.mp4  (5:18 for @nfu user only) \n cd2022_w16_2b_part1.mp4  (20:26 CoppeliaSim remote API 與場景影像串流介紹) \n cd2022_w16_2b_part2.mp4  (41:03 協同場景控制, 場景影像串流介紹) \n cd2022_w16_2b_part3.mp4  (26:22 4.3.0 CoppliaSim Visualizaton stream 功能介紹) \n cp2022 計算機程式 site \n cad2022 電腦輔助設計與實習 site \n Foundations of Robotics - A Multidisciplinary Approach with Python and ROS \n Sine Cosine Algorithm for Optimization \n https://github.com/luizaes/sca-algorithm \n https://github.com/topics/sine-cosine-algorithm \n https://github.com/Valdecy/Metaheuristic-Sine_Cosine_Algorithm \n A comprehensive survey of sine cosine algorithm: variants and applications \n Advances in Sine Cosine Algorithm: A comprehensive survey \n https://markus-x-buchholz.medium.com/the-sine-cosine-algorithm-for-solving-optimization-problems-with-constraints-in-c-530aecd0d77b \n https://github.com/markusbuchholz/The-Sine-Cosine-Algorithm-for-Solving-Optimization-Problems-with-Constraints-in-Cpp \n', 'tags': '', 'url': 'Ref.html'}, {'title': 'Talks', 'text': 'https://unboxingai.show/podcast-item/the-next-frontier-computer-vision-on-3d-data-with-or-litany/ \n https://youtu.be/RVFIDEuNtt0 \n', 'tags': '', 'url': 'Talks.html'}, {'title': 'Topics', 'text': '協同專案實習主題:  開發一款能在 web-based CoppeliaSim 場景中雙方或多方 (human or computer) 對玩的遊戲 (game) 產品 . \n 作業名稱 2a 或 2b 代表兩人一組的專題一作業, 各組 Team 名稱 pj1ag1 代表甲班 Project 1, 第一組專題分組, 而乙班的 Team 則使用 pj1bg1 作為第一組的團隊名稱. \n 請從 CoppeliaSim 的 模擬場景  Tutorial 進行練習. \n 作業名稱 4a 或 4b 代表四人一組的專題一作業, 各組 Team 名稱 pj2ag1 代表甲班 Project 2, 第一組專題分組, 而乙班的 Team 則使用 pj2bg1 作為第一組的團隊名稱. \n 作業名稱 8a 或 8b 代表八人一組的專題一作業, 各組 Team 名稱 pj3ag1 代表甲班 Project 3, 第一組專題分組, 而乙班的 Team 則使用 pj3bg1 作為第一組的團隊名稱. \n \n', 'tags': '', 'url': 'Topics.html'}, {'title': 'Classroom', 'text': '這裡採用 Github Classroom 建立各分組的專案倉儲, 其中牽涉兩人一組、四人一組與八人一組的倉儲 classroom repository. \n Github Classroom 的 原始碼開放至 Mar 12, 2020 , 之後轉為封閉源專案開發. 是一個採用  Ruby  編寫的 Web-based 程式套件. \n https://classroom.github.com \n Github Classroom 中的作業 (assignment) 分為 \n https://docs.github.com/en/education/manage-coursework-with-github-classroom/teach-with-github-classroom/create-an-individual-assignment \n 以及 \n https://docs.github.com/en/education/manage-coursework-with-github-classroom/teach-with-github-classroom/create-a-group-assignment \n Individual assignments (個人作業): \n An individual assignment is coursework for each student who participates in a course on GitHub Classroom. The student is intended to complete an individual assignment without help from other students. When a student accepts an assignment, GitHub Classroom automatically creates a new repository for the student. The repositories can be empty, or you can create the repositories from a template repository with starter code, documentation, tests, and other resources. Each assignment repository belongs to your organization account on GitHub. Each assignment has a title and an optional deadline. You can choose the visibility of repositories that GitHub Classroom creates and choose access permissions. You can also automatically grade assignments and create a dedicated space to discuss the assignment with the student. For a video demonstration of the creation of an individual assignment, see " Basics of setting up GitHub Classroom ." You can reuse existing assignments in any other classroom you have admin access to, including classrooms in a different organization. For more information, see " Reuse an assignment ." \n Group assignments (分組作業): \n A group assignment is collaborative coursework for groups of students on GitHub Classroom. Students can work together on a group assignment in a shared repository, like a team of professional developers. When a student accepts a group assignment, the student can create a new team or join an existing team. GitHub Classroom saves the teams for an assignment as a set. You can name the set of teams for a specific assignment when you create the assignment, and you can reuse that set of teams for a later assignment. For each group assignment, GitHub Classroom automatically creates a single shared repository for the team to access. The repository can be empty, or you can create the repository from a template repository with starter code, documentation, tests, and other resources. The repository belongs to your organization account on GitHub, and GitHub Classroom grants access to teams that students create or join when accepting the assignment. Each assignment has a title and an optional deadline. You can choose the visibility of repositories that GitHub Classroom creates and choose access permissions. You can also automatically grade assignments and create a dedicated space to discuss the assignment with the student. You can decide how many teams one assignment can have, and how many members each team can have. Each team that a student creates for an assignment is a team within your organization on GitHub. The visibility of the team is secret. Teams that you create on GitHub will not appear in GitHub Classroom. For more information, see " About teams ." For a video demonstration of the creation of a group assignment, see " Basics of setting up GitHub Classroom ." You can reuse existing assignments in any other classroom you have admin access to, including classrooms in a different organization. For more information, see " Reuse an assignment ." \n', 'tags': '', 'url': 'Classroom.html'}, {'title': 'Github', 'text': 'https://github.com/mdecourse-org/gitub_fundamental-scrum-1 \n', 'tags': '', 'url': 'Github.html'}, {'title': 'Personal', 'text': '個人 Resume 作業目標: \n \n 讓學員透過 GMail 或 Microsoft 帳號, 以個人手機門號登記 Openai 帳號, 可取得一次性 $18 的使用權限, 以 API 程式與 ChatGPT 對談. \n 讓學員整理個人的英文版 Resume. \n 讓學員了解除了 Word 之外, 也可以使用 LaTeX 程式產生 PDF 文件. \n 讓學員了解利用純文字表達文件格式的  LaTeX  檔案, 與 Github 結合後, 應該 比 Word 更適合用於協同專案 . \n \n 個人作業名稱為 resume, 以下列倉儲作為 Template: \n https://github.com/mdecycu/Resume-Automation-using-ChatGPT \n 利用 Github Classroom 建立個人作業, 以 Python、LaTeX 與 ChatGPT 協助建立個人 resume,  最終希望能用來協助建立個人網頁或分組專題報告 . \n Resume-Automation-using-ChatGPT 倉儲: \n \n 所提供的 Template 只支援英文, 可嘗試 利用 LaTeX 程式語法修改為中文 \n OpenAI 的使用, 可以利用 Gmail 帳號 申請 API key , 每一個手機門號只能取得第一個登錄帳號下的 $18 免費使用, 且必須在三個月內用完. 其中使用 Davinci\xa0 模型每 1k token 收費 $0.02. \n Resume 內容整理使用 Davinci 模型, 可在 免費額度 內進行測試 ( token  與 費用計算 ) \n 必須要在可攜程式環境中以  MikTeX  (約 800MB) 或  TeXLive  (約 2GB) 進行編譯 (納入 Portable MiKTeX 之後的 可攜 Python 程式套件:  wcm_portable_w2-4-miktex.7z ) \n Python 必須 安裝 openai module \n 利用  tkinter  建立的 GUI 介面表單輸入 Resume 欄位資料後, 交由 OpenAI \n \n 個人作業倉儲名稱: mdecd2023/resume-github_帳號, 例如:  https://github.com/mdecd2023/resume-scrum-1  就是帳號為 scrum-1 用戶所取得的 resume 作業倉儲. \n 作業討論: \n 採  Bing Microsoft Translator  直接將  Why LaTeX ? 翻譯如下: \n 為什麼選擇 LaTeX？ \n 對於包含多個或兩個數學表達式的文檔，LaTeX 為教師提供了比 Microsoft Word 等編輯器更多的優勢。 \n \n 在 LaTeX 中包含數學運算式涉及鍵入一些適當的字元。相比之下，在 Word 中包含數學需要公式編輯器，這是一個繁瑣且緩慢的圖形用戶介面。 \n 由於內容是在純文本檔中指定的，因此通常可以使用搜索和替換或其他節省時間的機制有效地處理在 Word 中費力的任務。 \n 由於上述原因，LaTeX 可節省大量長期運行時間。 \n 許多用戶發現，隨著時間的推移，LaTeX 中的排版變得越來越有趣和方便。相比之下，經驗表明，即使對於專家用戶來說，Word的挫敗感在很大程度上仍然存在。 \n 由於 LaTeX 是開源的，因此您可以輕鬆地與其他教師共用您的內容，而不會出現版本相容性或許可問題。 \n 由於 LaTeX 已經適應了數學和科學研究社區的需求，因此您不太可能希望在 LaTeX 中無法實現的符號。 \n 重複性任務可以更容易地自動化。例如，通過適當的設置，可以為學習時程表的學生 製作練習表 幾秒鐘。 \n LaTeX 足夠受歡迎，論壇和其他互聯網資源為那些對高級功能感興趣的人提供了良好的支援。 \n \n 值得一提的是幾個缺點: \n \n LaTeX 有一個學習曲線。這是放棄圖形使用者介面（如 Word 的公式編輯器）所帶來的效率的代價，起初成本肯定很大。 \n 如果您在代碼中犯了足夠重要的錯誤，LaTeX 將為您提供錯誤訊息而不是輸出檔。但是，調試時間會隨著經驗的增加而迅速減少。 \n \n 參考資料: \n \n LaTeX 使用中文 \n https://oz.nthu.edu.tw/~u9771040/TeX+LaTeX.pdf \n https://lvjr.bitbucket.io/tutorial/learn-latex.pdf \n 必學工具 LaTeX \n LaTeX 線上教材 \n LaTeX 的快速入門 \n 4072pj1 專題報告 \n 4072pj3 專題報告 \n pj4082 專題報告 \n LaTeX to PDF using Leo Editor \n \n OpenAI Python API 程式範例: \n # 導入 openai\nimport openai\n\n# 輸入 api_key\nopenai.api_key = "your_key"\n\n# 三個月內有 $18 可以進行測試 Davinci 0.02/1k token\n\n\n# 利用 create 建立問答\nresponse = openai.Completion.create(\nmodel="text-davinci-003",\nprompt="請列出不要讀機械設計系的 5 個理由",\nmax_tokens=256,\ntemperature=0,\ntop_p=1,\nfrequency_penalty=0,\npresence_penalty=0\n)\n\n# 傳回的 dict 中的 text\noutput = response["choices"][0]["text"]\nprint(output)\n\n"""\n1. 機械設計系的課程複雜，學習負擔大。\n2. 機械設計系的就業前景不明朗。\n3. 機械設計系的課程需要花費大量時間在實驗室中。\n4. 機械設計系的課程需要掌握大量的專業知識。\n5. 機械設計系的課程需要掌握大量的計算機軟件。\n""" \n wcm_portable_w2-4-miktex.7z  中的 MiKTeX 安裝 cjk 與 cjk-fonts 後, 可以利用: \n pdflatex -interaction=nonstopmode chinese_ex1.tex 將 chinese_ex1.tex 轉為 pdf 檔案. \n chinese_ex1.tex 內容如下: \n % 轉檔指令: pdflatex -interaction=nonstopmode chinese_ex1.tex\n\\documentclass[book, oneside, 12pt]{article}\n\\usepackage{CJKutf8}\n\\begin{document}\n%以下 bsmi 為明體, 若要使用楷書, 則使用 bkai\n\\begin{CJK*}{UTF8}{bsmi}\n\n\\section{標題}\n網際內容管理與協同產品設計實習 \\\\[18pt] % \\\\ will end the line [18pt] means next line spacing gap is 18pt, 1point ≈ 0.0138 inch ≈ 0.3515 mm\nWeb-based content management, collaborative product design.\n\n\\section{2023 Spring 學期}\n安裝 cjk 與 cjk-fonts 終於可以使用中文\n\n\\end{CJK*}\n\\end{document} \n 修改上述 Resume.tex 只要加入 \\usepackage{CJKutf8}, 並在 \\begin{document} 之後加上 \\end{document} 成為: \n \\begin{document}\n\\begin{CJK*}{UTF8}{bsmi} \n 以及在 \\end{document} 之前加入 \\end{CJK*} 成為: \n \\end{CJK*}\n\\end{document} \n Resume.tex 內文就可以加入中文, 並以 pdflatex -interaction=nonstopmode Resume.tex 轉為 Resume.pdf \n \n', 'tags': '', 'url': 'Personal.html'}, {'title': '維護倉儲', 'text': '利用 cmsimde 網際內容管理, 有兩種方法: \n \n 將 cmsimde 設為倉儲的子模組 - git submodule add  https://github.com/mdecycu/cmsimde.git  cmsimde \n 取得 cmsimde 原始碼之後, 直接放入 cmsimde 目錄 \n 希望直接執行動態網站, 就必須將 cmsimde 中的 up_dir 所有檔案複製到倉儲根目錄 \n \n 維護倉儲的兩種權限: \n \n classic token \n ssh \n 詳細設定請參考  https://mde.tw/content/Token%20and%20SSH.html \n \n 2a-pj1ag15  目前的版本 , 問題是在新增 cmsimde 目錄時, 誤將 cmsimde 中的 .git 一起放入 cmsimde 目錄中. \n 在近端將 cmsimde 目錄刪除後, 重新建立正確的 cmsimde 子目錄, 就可以修正錯誤. \n', 'tags': '', 'url': '維護倉儲.html'}, {'title': 'Group', 'text': '請將 CoppeliaSim  Tutorial1  練習結果與心得放入分組網站中的 tutorial1 頁面 \n 實習主題:  開發一款能在 web-based CoppeliaSim 場景中雙方或多方 (human or computer) 對玩的遊戲 (game) 產品 . \n 請從 CoppeliaSim 的 模擬場景  Tutorial 進行練習, 例如:  Tutorial1 \n demo 分組專案: \n 倉儲:  https://github.com/mdecd2023/2a-pj1agx \n 網站:  https://mdecd2023.github.io/2a-pj1agx \n git gui tool:  sourcetree.7z \n https://confluence.atlassian.com/get-started-with-sourcetree \n 參考: \n https://github.com/mdecycu/warehouse-robot \n https://github.com/stepjam/PyRep \n https://github.com/chauby/CoppeliaSimRL \n Efficient delivery of Robotics Programming educational content using Cloud Robotics  (2022) \n Learning resource - CoppeliaSim \n 分組作業: 分為 2 人一組 (作業名稱為 2a 或 2b, 專案名稱為 pj1), 4 人一組 (作業名稱為 4a 或 4b, 專案名稱為 pj2) 與 8 人一組 (作業名稱為 8a 或 8b, 專案名稱為 pj3) 之三階段分組作業, 其中第一階段可 2~3 人, 第二階段可最多 5 人, 第三階段可最多 9 人. \n 每一階段的作業都會對應到一組 URL, 其中各組組長登入 Github 帳號後, 透過作業 URL 連結可以建立各組的 Team (團隊), 其分組序號將依照組長座號, 以面對教室方向, 由右往左以及由前往後排序, (即以電腦教室中座位之列序與行序排序, 例如: 若二甲組長座位位於教室第一列的最右邊位置, 則其所組成的 Project 1 Team 名稱為 pj1ag1, 該組的倉儲名稱將為  mdecd2023/2a-pj1ag1 ). \n cd2023 各階段個人與分組倉儲均位於 organization 名稱為 mdecd2023 的帳號下:  https://github.com/mdecd2023/ \n 分組作業倉儲名稱: \n Project 1: 兩人一組的協同倉儲 \n mdecd2023/2a-pj1ag1 \n 2a 為甲班 2 人一組的第一個分組作業名稱, pj1ag1 則為甲班第一組組長配合 pj1 分組作業所建立的 pj1ag1 Team 名稱, 表示這是甲班第一組第一個至少 2 人一組, 至多 3 人的分組作業名稱. \n 而甲班第一組成員, 在登入 Github 帳號後, 可點選第一個分組作業連結後, 再選擇 joint pj1ag1, 加入甲班第一組進行協同設計. \n mdecd2023/2b-pj1bg1  則為二乙專案一的第一組分組倉儲名稱, 2b 為作業名稱, pj1bg1 則為乙班第一組組長配合第一個分組專案所建立的 Team 名稱 \n Project 2: 四人一組的協同倉儲 \n mdecd2023/4a-pj2ag1 \n mdecd2023/4b-pj2bg1 \n Project 3: 八人一組的協同倉儲 \n mdecd2023/8a-pj3ag1 \n mdecd2023/8b-pj3bg1 \n', 'tags': '', 'url': 'Group.html'}, {'title': 'pj1bgz', 'text': 'https://github.com/mdecd2023/2b-pj1bgz \n https://mdecd2023.github.io/2b-pj1bgz \n 利用上述倉儲, 以 cmsimde 單獨作為目錄, 以 SSH 協定取得管理權限, 並且納入協同 LaTeX 分組報告編寫機制的教學影片:  2b-pj1bgz 分組倉儲 SSH 與 LaTeX 分組報告製作教學.mp4 \n 利用 cmsimde 建立網頁: \n \n submodule - 比較方便跟官網版本同步 - cd cmsimde, git pull origin master \n 單獨作為目錄  - 可以自行維護程式碼 - 不可納入  cmsimde  的版本 \n 其他方法 \n \n 維護網頁(可以透過 token 或 SSH 取得維護權限): \n \n localhost  - 將倉儲 git clone 至近端, 然後以符合需求的可攜或安裝 Python 環境啟動 cmsimde/wsgi.py (可以下載\xa0 wcm_portable_w2-4-miktex.7z ) \n Replit - 可直接利用 web-based 維護動態與靜態網站內容 (給定的免費資源可能不符合使用) \n 自行安裝配置類似 Replit 的服務 ( stud2.cycu.org ) \n \n 針對 github 帳號尚未登錄至\xa0 https://github.com/mdecycu/studlist \xa0倉儲者, 可以利用 pull requests 進入更改流程: \n \n 登入個人的 github 帳號後 \n 連結至\xa0 https://github.com/mdecycu/studlist \xa0後, 以 fork 將此倉儲複刻到自己的帳號下 \n 納入自己學號後的 github 帳號後, 以 pull requests 要求管理者將改版內容檢查後合併 \n \n \n', 'tags': '', 'url': 'pj1bgz.html'}, {'title': 'Note', 'text': 'cd2023a w3 note \n \n 星期四 17:20 補課 \n 自製 Portable Python 流程 (可以隨身, 可以啟動不同權限, 可以配置不同模組) \n resume 作業, 以 pdflatex -interaction=nonstopmode Resume.tex (usb 產生錯誤, 重新設定可攜 MikTeX, 後續可嘗試利用 Github Actions 對 .tex 轉檔) \n 為何要使用 LaTeX, 可以確認內容的作者與時間,  https://gitbook.tw/chapters/using-git/git-blame \n pj1 兩人分組專案, 利用 Github Pages 轉出各分組網頁. \n 可採用先前的 cmsimde 設為 submodule (可配合原始 cmsimde 改版) \n 將 cmsimde 設為目錄, 可自行修改網際內容管理程式碼 \n 可採用其他方法建立分組網頁 \n 各分組逐步達成課程目標 - 協同專案實習主題:  開發一款能在 web-based CoppeliaSim 場景中雙方或多方 (human or computer) 對玩的遊戲 (game) 產品 . \n https://mde.tw/pjcopsim/content/bubbleRobTutorial.html \xa0 \n \n 各班各分組各組員評分項目紀錄在:  https://mdecd2023.github.io/2a-pj1agx \n Portable MiKTeX 製作方法: \n \n 從  https://miktex.org/download  下載 basic-miktex-22.10-x64.exe, 然後改名為 portable-miktex.exe 之後, 雙點擊之後開始進行可攜 MiKTeX 系統的安裝. \n 安裝後更新 MiKTeX 可以選擇  https://mirrors.cqu.edu.cn/  作為下載網站. \n 配置好可攜 MiKTeX 之後, 進入 Resume 倉儲 result 目錄執行: pdflatex -interaction=nonstopmofe Resume.tex, 就可以建立 Resume.pdf 履歷檔案. \n 接下來請各組設法將 template 與 MiKTeX 配置為可以轉換中文版的履歷檔案. \n \n 納入上列 Portable MiKTeX 之後的可攜 Python 程式套件:  wcm_portable_w2-4-miktex.7z \n cd2023a w4 note \n \n 在 CoppeliaSim 中, scene 與 model 有何不同? \n 在 CoppeliaSim scene 中, 何謂 clean model? \n 如何在 CoppeliaSim scene 中, 如何利用 Lua 以及 Python 建立 primitive type object, 並利用程式設定物件位置? \n CoppeliaSim .ttt 檔案格式與 .simscene.xml 檔案格式有何不同? \n \n', 'tags': '', 'url': 'Note.html'}, {'title': '評分', 'text': 'Personal resume: \n 請說明個人在 resume 個人作業分別完成的項目, 如何展示? \n w3: \n 第三週結束時仍 無分組網站 之分組: \n 2a: \n ag3, ag4, ag7, ag8, ag9, ag12, ag14, ag15, ag16, ag18, ag19, ag20 \n 2b \n bg3, bg8, bg9, bg28, bg29, bg30, bg31 \n w4: \n BubbleRub tutorial 製作結果與心得 presentation (以 Web-based 方式發表): \n Tutorial 議題: Why? How? What? (主題為何? 如何協同? 完成內容展示 - in person or in webpages) \n \n \n \n \n', 'tags': '', 'url': '評分.html'}, {'title': '模擬場景', 'text': 'https://mde.tw/pjcopsim/content/bubbleRobTutorial.html \n https://mde.tw/pjcopsim/content/buildingAModelTutorial.html \n https://mde.tw/pjcopsim/content/lineFollowingBubbleRobTutorial.html \n https://mde.tw/pjcopsim/content/inverseKinematicsTutorial.html \n https://mde.tw/pjcopsim/content/externalControllerTutorial.html \n https://mde.tw/pjcopsim/content/pluginTutorial.html \n https://mde.tw/pjcopsim/content/robotLanguageIntegrationTutorial.html \n https://mde.tw/pjcopsim/content/ros1Tutorial.html \n https://mde.tw/pjcopsim/content/ros2Tutorial.html \n https://mde.tw/pjcopsim/content/compilingCoppeliaSim.html \n', 'tags': '', 'url': '模擬場景.html'}, {'title': 'NX2027', 'text': "NX2027.3401_lite_cad2022.7z  for @nfu users only \n 目前 NX 最新版本為 NX2212.3001 (下載檔案為 12.75 GB) \n What's new in NX2212? \n", 'tags': '', 'url': 'NX2027.html'}, {'title': 'NX Server', 'text': 'Web based NX2027 Server \n', 'tags': '', 'url': 'NX Server.html'}, {'title': 'Onshape', 'text': "https://www.onshape.com/en/education/ \n Onshape Architecture:\xa0 https://youtu.be/kPNlzlkBGMA \xa0 \n 影片公開一年後, 只有不到 1200 views, why? 曲高和寡或者嘗試推動 MCAD 前進的 包袱實在太大 ? \n https://creativecloud.adobe.com/zh-Hant/discover/article/q-and-a-onshape-s-jon-hirschtick \n The Onshape acquisition - a closer look \n https://www.josh-ua.co/blog/2019/2/13/solidworks-vs-siemens-nx-vs-onshape \n What's new on 01.11, 2023?\xa0 https://youtu.be/ND_pwCCjMEQ \xa0 \n 如何在 Web-based 環境, 讓 human 與 machine 能夠在 手足球檯 上對陣, 且進球後以最快的方式回到球場. \n \n", 'tags': '', 'url': 'Onshape.html'}, {'title': 'Free CAD', 'text': '自由的電腦輔助設計環境: \n 從教育單位 協助產業界提前在校培訓學員使用特定商用電腦套件 的角度來說: \n 所有的教育版電腦套件都應該自由讓教育單位 免付費使用 , 並且儘可能地開源這些軟體程式, 假如他們真的將所謂 永續經營 當作一回事的話. \n https://opensource.siemens.com/events/2022/ \n https://www.engineering.com/story/new-leaders-in-forresters-plm-wave-why-siemens-will-be-a-tough-nut-to-crack-for-dassault-and-ptc \n Mechanical computer aided design software with freedom \n 自由的電腦輔助機械設計軟體 \n https://www.cadforum.net \n https://forum.freecad.org \n https://www.peerspot.com/categories/cad \n Creo for student:  https://www.ptc.com/en/products/education/free-software/creo-college-download \n Free AutoDesk Products:  https://www.autodesk.com/education/edu-software/overview \n IronCAD for free:  https://www.ironcad.com/student/ \n Ansys Student Versions:  https://www.ansys.com/academic/students \n', 'tags': '', 'url': 'Free CAD.html'}, {'title': 'Solid Edge', 'text': 'https://solidedge.siemens.com/en/solutions/users/hobbyists-and-makers/ \xa0 \n https://solidedge.siemens.com/en/free-software/overview/ \xa0 \n https://docs.plm.automation.siemens.com/docs/se/2020/api/webframe.html  (API) \n https://www.plm.automation.siemens.com/zh_cn/Images/Solid_Edge_API_tcm78-125829.pdf \n Solid Edge community edition: \n https://www.plm.automation.siemens.com/plmapp/education/solid-edge/zh_tw/free-software/community \xa0(下載約 4GB, 安裝後佔約 8GB, 授權使用時段: 06-jul-2021 至 31-dec-2025) \n 需要 Windows 10 Enterprise or Professional (64-bit only) version 20H2 or later, Windows 11 Enterprise or Professional version 21H2 or later,  https://developer.microsoft.com/en-us/microsoft-edge/webview2/ , MS .Net Framework 4.8, MS Visual C++ 2015-2019 redistributable X64 \n 會同時安裝\xa0 https://www.keyshot.com/ \xa011 版 \n SELicense.lic 位於 Preferences 目錄下 \n No Solid Edge Cloud Gateway \n 圖檔同教育版無法與商業版相容 \n 所建立的工程圖會有浮水印 \n Solid Edge Documentation \n SE2020 API Documentation \n 與 NX, Onshape 比較: \n 自帶使用授權, 可在不上網情況下使用 \n 安裝版 (下載 3.8GB, 安裝後佔 8GB), 適合在自己的電腦或筆電中安裝執行. \n 可攜版 (下載 4.28GB, 解開後佔 9.4GB), 適合在有防寫卡保護的電腦或虛擬主機中使用. \n 功能與 SolidWorks, Onshape 相當, 屬於中級 MCAD \n 在安裝防寫卡的電腦中使用 Solid Edge 2023: \n \n 因為系統並未安裝 Solid Edge, 因此可以使用已經安裝 Solid Edge 的 vdi 製作一個虛擬主機後使用 \n 配合 subst 指令, 設法在安裝 MS Edge Webview 與 VC 2015-2019 redistributable 後, import 所需的 .reg 後, 將 install 指令指向 .msi 並且設定 p_schema 與 se_license_server 等環境變數後執行 edge.exe 後啟動. 例如: \n \n start_se2023.bat \n @echo off\nset Disk=x\nsubst %Disk%: "data"\n\nset se_license_server=x:\\se2023\\Preferences\\SELicense.lic\nset p_schema=x:\\se2023\\Schema\nset sessionname=console\nREM set keyshot11=c:\\users\\public\\documents\\keyshot 11\nset keyshot_external_license_folder=x:\\se2023\\program\n\nstart x:\\se2023\\program\\edge.exe \n stop_se2023.bat \n @echo off\nset Disk=x\npath=%PATH%;\n\ntaskkill /IM edge.exe /F\n\nREM 終止虛擬硬碟與目錄的對應\nsubst %Disk%: /D\n\nEXIT \n 若在設定 X 虛擬槽之後, 關閉 Solid Edge 2023 後, 尚未執行 stop_se2023.bat 之前開啟 Solid Edge 2023, 則可以使用下列 start.bat \n set se_license_server=x:\\se2023\\Preferences\\SELicense.lic\nset p_schema=x:\\se2023\\Schema\nset sessionname=console\nREM set keyshot11=c:\\users\\public\\documents\\keyshot 11\nset keyshot_external_license_folder=x:\\se2023\\program\n\nstart x:\\se2023\\program\\edge.exe \n', 'tags': '', 'url': 'Solid Edge.html'}, {'title': 'SE Tutorial', 'text': 'Solid Edge 自學課程 \n Solid Edge 教學影片 \n Solid Edge 練習影片 \n intro to 3D CAD modeling \n Quick UI tour \n Create a part \n Create an assembly \n Create a drawing \n Simulation workflow \n More tutorial videos \n CAD Central \n https://www.scan2cad.com/blog/cad/solid-edge-basics/ \n https://www.digicad.fr/pdf/Solid-Edge-Synchronous-Technology-ebook-61045-tcm1023-254657.pdf \n Blogs: \n Solid Edge reloaded \n Dynamic Designer \n With ironpython ( ref ): \n     # Header\n    import clr\n    clr.AddReference("Interop.SolidEdge")\n    clr.AddReference("SolidEdge.Community")\n    clr.AddReference("System.Runtime.InteropServices")\n     \n    # Import Solid Edge Classes\n    import SolidEdgeFramework as seFramework\n    import SolidEdgePart as sePart\n    import SolidEdgeCommunity as seCommunity\n    import System.Runtime.InteropServices as SRI\n    from System import Console, Array\n     \n    # Solid Edge Objects\n    seApplication = None\n    seDocuments = None\n    sePartDocument = None\n     \n    # Start tasks here\n    try:\n        seCommunity.OleMessageFilter.Register()\n        seApplication = seCommunity.SolidEdgeUtils.Connect(True)\n        seDocuments = seApplication.Documents\n        sePartDocument = seDocuments.Add("SolidEdge.PartDocument")\n        \n    except :\n        print("Exception")\n    finally:\n        seCommunity.OleMessageFilter.Ungister() \n https://github.com/Rlee13/IronPythonExamplesForSE \n API: \n https://docs.plm.automation.siemens.com/docs/se/2020/api/webframe.html \n https://timshyue.com/category/c/solid-edge-api/ \n https://github.com/rmcanany/SolidEdgeSpy \n https://oneplm.com/wp-content/uploads/SEU2015_SolidEdgeAutomation.compressed.pdf \n', 'tags': '', 'url': 'SE Tutorial.html'}, {'title': 'DLL', 'text': '如何透過 Python 重用 Windows TLB or OCX 等 DLL 動態連結程式庫 \n 以下的 Python 程式採用  https://mde.tw/wcm2023/content/w2.html  中所建立的可攜 Python 程式套件: \n wcm_portable_w2-2.7z \n 並進行下列修改: \n \n 建立 home_ipv6 目錄下的 DeskTop 目錄 \n 編輯 python.properties 將 line 153 改為: python.command=pythonw \n 編輯 SciTEGlobal.properties 將 line 162 改為: load.on.activate=1 \n 編輯 SciTEGlobal.properties 將 line 204 改為: tabsize=4 \n 編輯 SciTEGlobal.properties 將 line 205 改為: indent.size=4 \n 編輯 SciTEGlobal.properties 將 line 335 # 拿掉 \n 編輯 SciTEGlobal.properties 將 line 336 以 # 蓋掉 \n \n 修改之後的可攜 Python:  wcm_portable_w2-3.7z \n 然後參考 與 GetSystemMetrics 有關的說明 , 執行下列程式: \n import ctypes\n\nuser32 = ctypes.WinDLL(\'User32.dll\')\nprint("目前螢幕設定寬:" + str(user32.GetSystemMetrics(0)))\nprint("目前螢幕設定長:" + str(user32.GetSystemMetrics(1))) \n 接著確認上述可攜系統已經安裝 pywin32: pip install pywin32 \n 就可以利用 python -m win32com.client.combrowse 瀏覽操作系統中的 COM 程式庫. \n 若要將 Solid Edge Type library 轉為 .py, 執行 python -m win32com.client.makepy -i \n Y:\\>python -m win32com.client.makepy -i\nSolid Edge Geometry Type Library\n {3E2B3BE1-F0B9-11D1-BDFD-080036B4D502}, lcid=0, major=1, minor=0\n >>> # Use these commands in Python code to auto generate .py support\n >>> from win32com.client import gencache\n >>> gencache.EnsureModule(\'{3E2B3BE1-F0B9-11D1-BDFD-080036B4D502}\', 0, 1, 0) \n 接著利用 Python 程式執行, 列出轉為 .py 後的 Type Library 所在位置: \n from win32com.client import gencache\n\ngeom_type_lib = gencache.EnsureModule(\'{3E2B3BE1-F0B9-11D1-BDFD-080036B4D502}\', 0, 1, 0)\nprint(dir(geom_type_lib))\nprint(repr(geom_type_lib)) \n 執行後可以取得 Solid Edge Geometry Type Library 轉出 .py 後的檔案所在目錄: \n <module \'win32com.gen_py.3E2B3BE1-F0B9-11D1-BDFD-080036B4D502x0x1x0\' from \'C:\\\\Users\\\\pj2023\\\\AppData\\\\Local\\\\Temp\\\\gen_py\\\\3.11\\\\3E2B3BE1-F0B9-11D1-BDFD-080036B4D502x0x1x0.py\'> \n 參考: \n Solid Edge Programming Guide.pdf  (for @nfu users only) \n', 'tags': '', 'url': 'DLL.html'}, {'title': 'Virtualization', 'text': 'https://www.hpe.com/emea_europe/en/what-is/application-virtualization.html \n https://www.hpe.com/emea_europe/en/what-is/hypervisor.html \n https://wiki.alquds.edu/?query=Application_Virtualization \n https://answers.microsoft.com/en-us/windows/forum/all/how-to-move-the-cusersusernameappdata-folder-to/35e0e881-6e66-4905-9f63-bd99477586b4 \n \n \n', 'tags': '', 'url': 'Virtualization.html'}, {'title': 'Femap', 'text': 'Femap is a finite element modeling and post-processing environment. It interacts with solvers such as Nastran to model physical behavior to assist in the drawing of conclusions about said behavior. \n 免費學生版 Femap 下載  (提供 Femap 2021.2 版) \n "這是一款對學生免費的專業 FEA 軟體 縱觀整個製造產業，降低成本和提高品質的壓力正不斷推動著人們對數字分析和分析技能的需求。具備進階工程分析的知識技能將為您進入專業隊伍提供巨大優勢。利用這款 Femap 學生版軟體，可獲得各種所需的技能，因為該免費版具有市面上最先進的工程分析環境。 此免費下載版： \xa0\xa0\xa0 可供任何現時學生使用 \xa0\xa0\xa0 用於學術課程 \xa0\xa0\xa0 授權不會過期" \n Femap Student Edition \n "Free professional FEA software for students Across the manufacturing industry, pressure to reduce costs and improve quality is driving demand for digital simulation and analysis skills. Knowledge of advanced engineering analysis can give you a huge advantage as you enter the professional workforce. You can build the skillset you need with Femap Student Edition software—a free version of the most advanced engineering analysis environment on the market. This free download: \xa0\xa0\xa0 Is available to any active student \xa0\xa0\xa0 Is intended for academic course work \xa0\xa0\xa0 Has a license that will not expire" \n https://github.com/jjcremmers/PyFEM \n https://community.sw.siemens.com/s/article/writing-the-femap-api-in-python \n https://structures.aero/webinar/femap-api-integrating-python/ \n https://www.ata-e.com/wp-content/uploads/2021/10/UsingFemapAPI_20150623.pdf \n https://www.femto.eu/wp-content/uploads/2020/01/3.-Quick-guide-to-Femap-API-Python.pdf \n https://community.sw.siemens.com/s/question/0D54O00007Iqhf7SAB/api-to-create-geometry-and-mesh-around-weld-line \n', 'tags': '', 'url': 'Femap.html'}, {'title': 'NGSolve', 'text': 'Portable Python 3.11.2 for NGSolve project downloads \n 網路連線至 Server 品質較慢時: \n pip --default-timeout=1000 install ngsolve \n pip --default-timeout=1000 install numpy \n pip --default-timeout=1000 install webgui_jupyter_widgets \n pip --default-timeout=1000 install jupyter \n pip --default-timeout=1000 install scipy matplotlib \n jupyter nbextension install --user --py widgetsnbextension \n jupyter nbextension enable --user --py widgetsnbextension \n jupyter nbextension install --user --py webgui_jupyter_widgets \n jupyter nbextension enable --user --py webgui_jupyter_widgets \n 設定 Jupyter 登入密碼: \n jupyter notebook password \n cd i-tutorials \n jupyter-notebook index.ipynb \n 使用  http://localhost:8888  並以密碼登入 \n https://docu.ngsolve.org/nightly/i-tutorials/index.html \n https://github.com/NGSolve/ngsgui/blob/master/src/gui.py \n from ngsolve import *\nfrom ngsolve.webgui import Draw\n# https://github.com/NGSolve/ngsgui/blob/master/src/gui.py\nflat = occ.OCCGeometry("Y:/i-tutorials/191_SD_Flat.stp")\nmesh = Mesh(flat.GenerateMesh(maxh=5)).Curve(3)\nDraw (mesh); \n https://github.com/NGSolve/ngsgui  Pyside2 need Python < 3.11 \n ngsgui \n Y:\\Python3108\\Lib\\site-packages\\jupyter_client\\multikernelmanager.py fix \n File "y:\\Python3108\\Lib\\site-packages\\jupyter_client\\multikernelmanager.py", line 218, in _async_start_kernel \xa0\xa0\xa0 if km.ready.exception(): asyncio.exceptions.InvalidStateError: Exception is not set. \n             # fixed according to https://stackoverflow.com/questions/73887749/python-why-asyncio-exceptions-invalidstateerror-exception-is-not-set-here by Yen\n            if km.ready.done():\n              if km.ready.exception():\n                  raise km.ready.exception()  # type: ignore \n \n', 'tags': '', 'url': 'NGSolve.html'}, {'title': 'Solvespace', 'text': 'https://github.com/solvespace/solvespace \n Solvespace Forum \n', 'tags': '', 'url': 'Solvespace.html'}, {'title': 'Pyslvs-UI', 'text': 'https://github.com/KmolYuan/Pyslvs-UI \n', 'tags': '', 'url': 'Pyslvs-UI.html'}, {'title': 'FreeCAD', 'text': 'https://www.freecad.org/ \n', 'tags': '', 'url': 'FreeCAD.html'}, {'title': 'LaTeX', 'text': 'TEX is the typesetting language written by  Donald Knuth . He wrote a format of TEX called Plain TEX, but many people find Plain TEX complicated, so  Leslie Lamport  wrote a format of TEX called LATEX to make it a bit easier to use. You can think of LATEX as a go-between converting your instructions into TEX. \n 新手入門 LaTeX: \n https://www.dickimaw-books.com/latex/novices/novices-screen.pdf \n 在協同產品設計流程中, 各種資料的整理若採分散式版次管理模式(cd2023 將介紹 git 與  Fossil SCM  等兩種工具), 將可有效掌握各組員在設計過程中參與專案的角色與所貢獻的內容. 而此一協同設計架構也能用於編寫簡報或專案報告. 在此將採用 LaTeX 編寫設計過程中的各種技術文件及導引. \n 以 cmsimde 建立 .tex 協同設計文字檔案: \n 在 cmsimde 網際內容管理系統中, Python 可以利用 bs4 將 content 目錄中的  html 轉為 text , 之後若要放入 .tex 運用, 則必須再進行檔案編修. \n 另外, 則可直接透過 cmsimde 的 editor 寫出所需要的 latex/file.tex, 也就是直接利用網際編輯器處理 .tex 檔案內容, 並且配合 images 中的圖檔, 以 Github Actions 轉為 .pdf \n 協同分組報告設定檔案:  https://mdecd2023.github.io/resume-scrum-1/downloads/latex_cd_report_using_github_actions.7z\xa0 \n \\quad Defined in: LATEX Kernel. Horizontal spacing command equal to the current font’s em value \n The preamble is the part of the source code that comes after the \\documentclass command and before \\begin{document} (the start of the document environment). Only a few special commands may be placed in the preamble (such as \\title), and there are a few special commands that may only go in the preamble (such as \\usepackage). Nothing that generates text (for example, \\maketitle) may go in the preamble. \n pt TEX point: 72.27pt = 1in in inch: 1in = 25.4mm mm millimetre: 1mm=2.845pt cm centimetre: 1cm = 10mm ex the "x-height" of the current font em the width of a "quad" in the current font sp scaled point: 1sp = 65536pt bp big point (or PostScript point): 72bp = 1in dd didôt point: 1dd=0.376mm pc pica: 1pc=12pt cc cicero: 1cc=12dd mu math unit: 18mu = 1em \n References: \n https://latex.js.org/ \n https://www.swiftlatex.com \n \n', 'tags': '', 'url': 'LaTeX.html'}, {'title': 'CoppeliaSim', 'text': 'https://www.coppeliarobotics.com/ \n Download CoppeliasimEdu_4.3.0rev12.7z  (for @nfu users only) \n', 'tags': '', 'url': 'CoppeliaSim.html'}, {'title': 'BubbleRub', 'text': '有關 CoppelisSim 的 Web-based 使用者手冊:  https://mde.tw/content/pjcopsim.html \n BubbleRub 的導引文件: \n https://mde.tw/pjcopsim/content/bubbleRobTutorial.html \n This tutorial will introduce quite many CoppeliaSim functionalities while designing the simple mobile robot  BubbleRob . The CoppeliaSim scene file related to this tutorial is located in  scenes/tutorials/BubbleRob . \n Since this tutorial will fly over many different aspects, make sure to also have a look at the  other tutorials , mainly the  tutorial about building a simulation model . First of all, freshly start CoppeliaSim. The simulator displays a default  scene . We will start with the body of  BubbleRob . \n Building a (clean) simulation model : building a nice looking, fast displaying, fast simulating and stable simulation model. \n Scene: \n Compared to models, a scene can contain exactly the same type of elements, but additionally also includes following elements, specific to scenes: \n 1. The environment 2. The main script 3. Pages and views A scene or scene image content can be seen through a viewable object associated with a view, itself contained in a page. When creating a new scene ([Menu bar --> File --> New Scene]), the  default scene  will contain following elements: \n 1.  Several camera objects : cameras allow to see the scene if they are associated with a view. 2. Several light objects: without a light the scene would be hardly visible. The light is used to illuminate the scene. 3.  Several views : a view is associated with a camera and displays what the camera sees. Views are contained in pages. 4.  Several pages : a page contains one or several views. 4.  The environment : the environment is composed by properties as ambient light, fog, background color, etc. 6.  The floo r: the floor is made-up by objects grouped in a model. 7.  The default main script  (位於 lua/defaultMainScript.lua): the default main script should allow running minimal simulations, without the need of child scripts. A child script copied into the scene at a later stage will then also be automatically executed (called by the main script) if it is associated with a scene object. Scenes can be opened (loaded) with [Menu bar --> File --> Open Scene...] and saved with [Menu bar --> File --> Save Scene] or [Menu Bar --> File --> Save Scene as]. Scene files ("*.ttt"-files) also support drag and drop operations between the explorer window and the application window. Scene files can also be double-clicked, in which case they will launch the CoppeliaSim application and be opened. \n BubbleRob body \n We add a primitive sphere of diameter 0.2 to the scene with [Menu bar --> Add --> Primitive shape --> Sphere]. We adjust the X-size item to 0.2, then click OK. The created sphere will appear in the  visibility layer  1 by default, and be  dynamic and respondable  (since we kept the item Create dynamic and respondable shape enabled). This means that  BubbleRob\'s  body will be falling and able to react to collisions with other respondable shapes (i.e. simulated by the physics engine). We can see this is the  shape dynamics properties : items Body is respondable and Body is dynamic are enabled. We start the simulation (via the toolbar button, or by  pressing <control-space>  in the scene window), and copy-and-paste the created sphere (with [Menu bar --> Edit --> Copy selected objects] then [Menu bar --> Edit -> Paste buffer], or with <control-c> then <control-v>): the two spheres will react to collision and roll away. We stop the simulation: the duplicated sphere will automatically be removed. This default behaviour can be modified in the  simulation dialog (default Main settings: Pause on script error, Reset scene to initial state, and Remove new objects). \n We also want the  BubbleRob\'s  body to by usable by the other calculation modules (e.g.  distance calculation ). For that reason, we enable  Collidable ,  Measurable  and  Detectable  in the  object common properties  for that shape, if not already enabled. If we wanted, we could now also change the visual appearance of our sphere in the  shape properties . \n Now we open the  position dialog  on the translation tab, select the sphere representing  BubbleRob\'s  body, and enter 0.02 for Along Z. We make sure that the Relative to-item is set to World. Then we click Translate selection. This translates all selected objects by 2 cm along the absolute Z-axis, and effectively lifted our sphere a little bit. In the  scene hierarchy , we double-click the sphere\'s alias, so that we can edit it. We enter  bubbleRob  and press enter. \n \n Proximity sensor \n Next we will add a  proximity sensor  so that  BubbleRob  knows when it is approaching obstacles: we select [Menu bar --> Add --> Proximity sensor --> Cone type]. In the  orientation dialog  on the orientation tab, we enter 90 for Around Y and for Around Z, then click Rotate selection. In the  position dialog , on the position tab, we enter 0.1 for X-coord. and 0.12 for Z-coord. The proximity sensor is now correctly positioned relative to  BubbleRob\'s  body. We double-click the proximity sensor\'s icon in the  scene hierarchy  to open  its properties  dialog. We click Show volume parameter to open the  proximity sensor volume dialog . We adjust items Offset to 0.005, Angle to 30 and Range to 0.15. Then, in the  proximity sensor properties , we click Show detection parameters. This opens the  proximity sensor detection parameter dialog . We uncheck item Don\'t allow detections if distance smaller than then close that dialog again. In the scene hierarchy, we double-click the proximity sensor\'s alias in order to edit it. We enter  sensingNose  and press enter. \n We select  sensingNose , then control-select  bubbleRob , then click [Menu bar --> Edit --> Make last selected object parent]. This attaches the sensor to the body of the robot. We could also have dragged  sensingNose  onto  bubbleRob  in the scene hierarchy. \n \n Wheels \n Next we will take care of  BubbleRob\'s  wheels. We create a new scene with [Menu bar --> File --> New scene]. It is often very convenient to work across several scenes, in order to visualize and work only on specific elements. We add a pure primitive cylinder with dimensions (0.08,0.08,0.02). As for the body of  BubbleRob , we enable  Collidable ,  Measurable  and  Detectable  in the  object common properties  for that cylinder, if not already enabled. Then we set the cylinder\'s absolute position to (0.05,0.1,0.04) and its absolute orientation to (-90,0,0). We change the alias to  leftWheel . We copy and paste the wheel, and set the absolute Y coordinate of the copy to -0.1. We rename the copy to  rightWheel . We select the two wheels, copy them, then switch back to scene 1, then paste the wheels. \n \n Joints \n We now need to add  joints  (or motors) for the wheels. We click [Menu bar --> Add --> Joint --> Revolute] to add a revolute joint to the scene. Most of the time, when adding a new object to the scene, the object will appear at the origin of the world. We Keep the joint selected, then control-select  leftWheel . In the  position dialog , on the position tab, we click the Apply to selection button: this positioned the joint at the center of the left wheel. Then, in the  orientation dialog , on the orientation tab, we do the same: this oriented the joint in the same way as the left wheel. We rename the joint to  leftMotor . We now double-click the joint\'s icon in the scene hierarchy to open the  joint properties  dialog. Then we click Show dynamic parameters to open the  joint dynamics properties  dialog. We enable the motor, and check item Lock motor when target velocity is zero. We now repeat the same procedure for the right motor and rename it to  rightMotor . Now we attach the left wheel to the left motor, the right wheel to the right motor, then attach the two motors to  bubbleRob . This is what we have: \n \n Simulation - first run \n We run the simulation and notice that the robot is falling backwards. We are still missing a third contact point to the floor. We now add a small slider (or caster). In a new scene we and add a pure primitive sphere with diameter 0.05 and make the sphere  Collidable ,  Measurable  and  Detectable  (if not already enabled), then rename it to  slider . We set the Material to  noFrictionMaterial  in the  shape dynamics properties . To rigidly link the slider with the rest of the robot, we add a  force sensor object  with [Menu bar --> Add --> Force sensor]. We rename it to  connection  and shift it up by 0.05. We attach the slider to the force sensor, then copy both objects, switch back to scene 1 and paste them. We then shift the force sensor by -0.07 along the absolute X-axis, then attach it to the robot body. If we run the simulation now, we can notice that the slider is slightly moving in relation to the robot body: this is because both objects (i.e.  slider  and  bubbleRob ) are colliding with each other. To avoid strange effects during dynamics simulation, we have to inform CoppeliaSim that both objects do not mutually collide, and we do this in following way: in the  shape dynamics properties , for  slider  we set the local respondable mask to 00001111, and for  bubbleRob , we set the local respondable mask to 11110000. If we run the simulation again, we can notice that both objects do not interfere anymore. \n bubbleRob1.ttt  (velocity 100 for both wheel) \n bubbleRob1_zero_vel.ttt \n buttleRob1_threaded_lua.ttt \n Run the simuation again \n We run the simulation again and notice that  BubbleRob  slightly moves, even with locked motor. We also try to run the simulation with different physics engines: the result will be different. Stability of dynamic simulations is tightly linked to masses and inertias of the involved non-static shapes. For an explanation of this effect, make sure to carefully read  this  and  that  sections. We now try to correct for that undesired effect. We select the two wheels and the slider, and in the shape dynamics dialog we click three times M=M*2 (for selection). The effect is that all selected shapes will have their masses multiplied by 8. We do the same with the inertias of the 3 selected shapes, then run the simulation again: stability has improved. In the joint dynamics dialog, we set the Target velocity to 50 for both motors. We run the simulation:  BubbleRob  now moves forward and eventually falls off the floor. We reset the Target velocity item to zero for both motors. \n \n Create model \n The object  bubbleRob  is at the base of all  objects  that will later form the  BubbleRob   model . We will define the model a little bit later. Next we are going to add a  graph object  to  BubbleRob  in order to display its clearance distance. We click [Menu bar --> Add --> Graph] and rename it to  graph . We attach the graph to  bubbleRob , and set the graph\'s absolute coordinates to (0,0,0.005). \n Now we set one motor target velocity to 50, run the simulation, and will see  BubbleRob\'s  trajectory displayed in the scene. We then stop the simulation and reset the motor target velocity to zero. \n \n Add obstacle \n We add a pure primitive cylinder with following dimensions: (0.1, 0.1, 0.2). We want this cylinder to be static (i.e. not influenced by gravity or collisions) but still exerting some collision responses on non-static respondable shapes. For this, we disable Body is dynamic in the  shape dynamics properties . We also want our cylinder to be  Collidable ,  Measurable  and  Detectable . We do this in the  object common properties . Now, while the cylinder is still selected, we click the object translation toolbar button: \n Now we can drag any point in the scene: the cylinder will follow the movement while always being constrained to keep the same Z-coordinate. We copy and paste the cylinder a few times, and move them to positions around  BubbleRob  (it is most convenient to perform that while looking at the scene from the top). During object shifting, holding down the shift key allows to perform smaller shift steps. Holding down the ctrl key allows to move in an orthogonal direction to the  regular  direction(s). When done, select the camera pan toolbar button again: \n We set a target velocity of 50 for the left motor and run the simulation: the graph view now displays the distance to the closest obstacle and the distance segment is visible in the scene too. We stop the simulation and reset the target velocity to zero. \n We now need to finish BubbleRob as a  model  definition. We select the model base (i.e. object  bubbleRob ) then check Object is model base in the  object common properties : there is now a stippled bounding box that encompasses all objects in the model hierarchy. We select the two joints, the proximity sensor and the graph, then enable item Ignored by model bounding box and click Apply to selection, in the same dialog: the model bounding box now ignores the two joints and the proximity sensor. Still in the same dialog, we disable camera visibility layer 2, and enable camera visibility layer 10 for the two joints and the force sensor: this effectively hides the two joints and the force sensor, since layers 9-16 are disabled by default. At any time we can  modify the visibility layers for the whole scene . To finish the model definition, we select the vision sensor, the two wheels, the slider, and the graph, then enable item Select base of model instead: if we now try to select an object in our model in the scene, the whole model will be selected instead, which is a convenient way to handle and manipulate the whole model as a single object. Additionally, this protects the model against inadvertant modification. Individual objects in the model can still be selected in the scene by click-selecting them with control-shift, or normally selecting them in the scene hierarchy. We finally collapse the model tree in the scene hierarchy. \n \n Vision sensor \n Next we will add a  vision sensor , at the same position and orientation as  BubbleRob\'s  proximity sensor. We open the model hierarchy again, then click [Menu bar --> Add --> Vision sensor --> Perspective type], then attach the vision sensor to the proximity sensor, and set the local position and orientation of the vision sensor to (0,0,0). We also make sure the vision sensor is not not visible, not part of the model bounding box, and that if clicked, the model will be selected instead. In order to customize the vision sensor, we open  its properties  dialog. We set the Far clipping plane item to 1, and the Resolution x and Resolution y items to 256 and 256. We add a floating view to the scene, and over the newly added floating view, right-click [Popup menu --> View --> Associate view with selected vision sensor] (we make sure the vision sensor is selected during that process). \n We attach a child script to the vision sensor by clicking [Menu bar --> Add --> Associated child script --> Non threaded]. We double-click the icon that appeared next to the vision sensor in the scene hierarchy: this opens the child script that we just added. We copy and paste following code into the  script editor , then close it: \n function sysCall_vision(inData)\n    simVision.sensorImgToWorkImg(inData.handle) -- copy the vision sensor image to the work image\n    simVision.edgeDetectionOnWorkImg(inData.handle,0.2) -- perform edge detection on the work image\n    simVision.workImgToSensorImg(inData.handle) -- copy the work image to the vision sensor image buffer\nend\n \nfunction sysCall_init()\nend \n To be able to see the vision sensor\'s image, we start the simulation, then stop it again. \n The last thing that we need for our scene is a small  child script  that will control  BubbleRob\'s  behavior. We select  bubbleRob  and click [Menu bar --> Add --> Associated child script --> Non threaded]. We double-click the script icon that appeared next to  bubbleRob\'s  alias in the scene hierarchy and copy and paste following code into the  script editor , then close it: \n function speedChange_callback(ui,id,newVal)\n    speed=minMaxSpeed[1]+(minMaxSpeed[2]-minMaxSpeed[1])*newVal/100\nend\n \nfunction sysCall_init()\n    -- This is executed exactly once, the first time this script is executed\n    bubbleRobBase=sim.getObject(\'.\') -- this is bubbleRob\'s handle\n    leftMotor=sim.getObject("./leftMotor") -- Handle of the left motor\n    rightMotor=sim.getObject("./rightMotor") -- Handle of the right motor\n    noseSensor=sim.getObject("./sensingNose") -- Handle of the proximity sensor\n    minMaxSpeed={50*math.pi/180,300*math.pi/180} -- Min and max speeds for each motor\n    backUntilTime=-1 -- Tells whether bubbleRob is in forward or backward mode\n    robotCollection=sim.createCollection(0)\n    sim.addItemToCollection(robotCollection,sim.handle_tree,bubbleRobBase,0)\n    distanceSegment=sim.addDrawingObject(sim.drawing_lines,4,0,-1,1,{0,1,0})\n    robotTrace=sim.addDrawingObject(sim.drawing_linestrip+sim.drawing_cyclic,2,0,-1,200,{1,1,0})\n    graph=sim.getObject(\'./graph\')\n    distStream=sim.addGraphStream(graph,\'bubbleRob clearance\',\'m\',0,{1,0,0})\n    -- Create the custom UI:\n        xml = \'<ui title="\'..sim.getObjectAlias(bubbleRobBase,1)..\' speed" closeable="false" resizeable="false" activate="false">\'..[[\n        <hslider minimum="0" maximum="100" onchange="speedChange_callback" id="1"/>\n        <label text="" style="* {margin-left: 300px;}"/>\n        </ui>\n        ]]\n    ui=simUI.create(xml)\n    speed=(minMaxSpeed[1]+minMaxSpeed[2])*0.5\n    simUI.setSliderValue(ui,1,100*(speed-minMaxSpeed[1])/(minMaxSpeed[2]-minMaxSpeed[1]))\nend\n \nfunction sysCall_sensing()\n    local result,distData=sim.checkDistance(robotCollection,sim.handle_all)\n    if result>0 then\n        sim.addDrawingObjectItem(distanceSegment,nil)\n        sim.addDrawingObjectItem(distanceSegment,distData)\n        sim.setGraphStreamValue(graph,distStream,distData[7])\n    end\n    local p=sim.getObjectPosition(bubbleRobBase,-1)\n    sim.addDrawingObjectItem(robotTrace,p)\nend\n \nfunction sysCall_actuation()\n    result=sim.readProximitySensor(noseSensor) -- Read the proximity sensor\n    -- If we detected something, we set the backward mode:\n    if (result>0) then backUntilTime=sim.getSimulationTime()+4 end\n \n    if (backUntilTime<sim.getSimulationTime()) then\n        -- When in forward mode, we simply move forward at the desired speed\n        sim.setJointTargetVelocity(leftMotor,speed)\n        sim.setJointTargetVelocity(rightMotor,speed)\n    else\n        -- When in backward mode, we simply backup in a curve at reduced speed\n        sim.setJointTargetVelocity(leftMotor,-speed/2)\n        sim.setJointTargetVelocity(rightMotor,-speed/8)\n    end\nend\n \nfunction sysCall_cleanup()\n    simUI.destroy(ui)\nend \n We run the simulation.  BubbleRob  now moves forward while trying to avoid obstacles (in a very basic fashion). While the simulation is still running, change  BubbleRob\'s  velocity, and copy/paste it a few times. Also try to scale a few of them while the simulation is still running. Be aware that the minimum distance calculation functionality might be heavily slowing down the simulation, depending on the environment. \n Using a script to control a robot or model is only one way of doing. CoppeliaSim offers many different ways (also combined), have a look at the  external controller tutorial .', 'tags': '', 'url': 'BubbleRub.html'}, {'title': 'Settings', 'text': "User settings \n Some values and settings in CoppeliaSim are not dependent on a  scene  or  model , but rather dependent on the user. The user settings dialog (which partially reflects the content of file  system/usrset.txt ) can be accessed with [Menu bar --> Tools --> Settings] or by clicking following toolbar button: \n \n [Settings toolbar button] \n \n \n [User settings dialog] \n \n \n Translation step size: the linear step size used when translating  objects  in the  object manipulation mode . It is recommended to keep a value of 5 cm. Objects can be assigned specific step sizes in the  coordinate and transformation dialog . \n Rotation step size: the angular step size used when rotating  objects  in the  object manipulation mode . Objects can be assigned specific step sizes in the  coordinate and transformation dialog . \n Remove identical vertices, tolerance: when selected, vertices that lay nearby other vertices will be grouped to form one single vertex (that will then be shared among surrounding triangles). This reduces the amount of memory resources needed. This parameter is affecting meshes when they are being  imported  or  shapes  when leaving the  shape edit mode . Tolerance specifies the distance threshold to consider when grouping vertices. In general, keep a low value, but different from zero: some mesh data formats (e.g. STL) assign individual vertices to each triangle, regardless whether a vertex is identical with another vertex in another triangle; this can considerably increase the amount of memory required. \n Remove identical triangles: when selected, identical triangles in a mesh resource will be removed during an import operation or when a shape leaves the shape edit mode. \n Ignore triangle winding: a triangle can have two distinct orientations (since it has two distinct faces). When this item is selected, the triangle orientation is ignored when identifying identical triangles. \n Undo/redo enabled: enables or disables the undo/redo functionality. That functionality operates by serializing (saving) the entire scene to memory every time a change was registered. Only differences with previous undo points are memorized so as to use little memory. This is a very efficient and fail-safe undo/redo that is also supported from  plugins  or the  main client application , and that is permitted only because CoppeliaSim's serialization routines are very fast. There are however times when either the computer is really old or the scene content extremely large (e.g. very detailed cad data) where this method slows down the whole application. In that case, simply disable the undo/redo functionality. \n Display world reference: displays a small world reference frame in the lower left hand side of a  camera  view. As everywhere in CoppeliaSim, the red, green and blue arrows correspond to the x-, y-, and z-axis respectively. \n Display bounding boxes of selected objects: displays a white/yellow bounding box around selected objects \n Auto-save enabled: when auto-save is enabled, each opened scene will be saved on a regular basis. In case of a crash, the auto-saved scenes can be restored. The auto-save delay can be adjusted in the file system/usrset.txt. \n Hide console window: allows hiding or showing the  console window . By default, the console window is hidden. On Mac, this item is not available (the standard output can be viewed in the system console (Applications/Utilities/Console)). \n Hide hierarchy during simul.: automatically hides the scene hierarchy when simulation is running. \n Adjust OpenGL settings: opens a dialog that allows to adjust most settings related to OpenGL: \n \n   \n \n OpenGL settings \n \n [OpenGL settings dialog] \n \n \n Offscreen context type: the type of the offscreen rendering contexts. Default are invisible windows. \n FBO type: the type of the frame buffer objects. Default are Qt-based FBOs on Mac, and Non-Qt-based FBOs on Windows and Linux. \n VBO operation: whether Vertex Buffer Objects are used. The default option uses VBOs. \n Idle fps: the number of frames per second when idle. \n \n \n", 'tags': '', 'url': 'Settings.html'}, {'title': 'Interface', 'text': 'User interface \n The CoppeliaSim application is composed by several elements. Its main elements are: \n \n \n a console window: under Windows, when the CoppeliaSim application starts, a console window is created but directly hidden again. This default behavior of hiding the console window can be altered in the  user settings dialog . Under Linux, CoppeliaSim needs to be started from the console, which stays visible throughout the whole CoppeliaSim session. Under MacOSX, best is to start CoppeliaSim from a terminal, in order to have messages visible. The console or terminal window displays what  plugins  were loaded and whether their initialization procedure was successful. The console window is not interactive and is only used to output information. The user can directly output information to the console window with the  print  command (from within a script), or with the C  printf  or  std::cout  commands from within a plugin. In addition to that, the user can programmatically create  auxiliary console windows  to display information specific to a simulation for instance. \n an application window: the application window is the application\'s main window. It is used to display, edit, simulate and interact with a scene. The left and right mouse buttons, the mouse wheel as well as the keyboard have specific functions when activated in the application window. Within the application window the functions of the input devices (mouse and keyboard) may vary on context or activation location. \n several dialogs: next to the application window, the user can also edit and interact with a scene by adjusting dialog settings or parameters. Each dialog groups a set of related functions, or functions that apply to a same target object. A dialog\'s content might be context sensitive (e.g. dependent on the object selection state). \n \n \n Following illustrates a typical view of the CoppeliaSim application: \n \n [User interface elements] \n \n When you launch the CoppeliaSim application, CoppeliaSim will initialized one default  scene . The user is free to open several scenes in parallel. Each scene shares the application window and the dialogs with the other scenes, but only the active scene content will be visible in the application window or the dialogs (only one scene is visible at a given time). \n In following section, a brief description will be given of the application window\'s elements. For details about dialogs, refer to the related pages in this reference manual. \n \n \n application bar: the application bar indicates the type of license of your CoppeliaSim copy, the filename of the scene that is currently being displayed, the time used for one rendering pass (one display pass), and the simulator\'s current state (simulation state or type of the active edit mode). The application bar, as well as any surface within the application window, can also be used to drag-and-drop CoppeliaSim related files into the scene. Supported files include "*.ttt"-files (CoppeliaSim scene files) and "*.ttm"-files (CoppeliaSim model files). \n menu bar: the menu bar allows accessing almost all functionalities of the simulator. Most of the time, the items in the menu bar activate a dialog. The menu bar content is context-sensitive (i.e. it will depend on the current state of the simulator). Most functions in the menu bar can also alternatively be accessed through a popup menu, a double-click on an icon in the scene hierarchy view, or through a click of a toolbar button. \n toolbars: the toolbars present functions that are often accessed (e.g. changing the navigation mode, selecting another page, etc.). Some functions in toolbar 1, and all functions in toolbar 2 can also be accessed through the menu bar or popup menu. See further down for more details. Both toolbars can be docked and undocked, but docking works only with their respective initial positions. Following figure explains each toolbar button\'s function: \n \n \n \n [Toolbar 1] \n \n \n [Toolbar 2] \n \n \n model browser: the model browser is visible by default, but can be toggled with its corresponding toolbar button. It displays in its upper part a CoppeliaSim model folder structure, and in its lower part, thumbnails of  models  contained in the selected folder. Thumbnails can be dragged-and-dropped into the scene to automatically load the related model. Caught thumbnails appears dark if the drop area is not supported or not appropriate. \n \n \n \n [Model browser] \n \n \n scene hierarchy: the scene hierarchy is visible by default, but can be toggled with its corresponding toolbar button. It displays the content of a scene (i.e. all scene objects composing a scene). Since  scene objects  are built in a hierarchy-like structure, the scene hierarchy displays a tree of this hierarchy, and individual elements can be expanded or collapsed. A double-click on an icon opens/closes a property dialog related to the clicked icon. A double-click on an object alias allows editing it. The mouse wheel as well as a drag of the scene hierarchy view\'s scrollbars allows shifting the content up/down or left/right. Control and shift selection is always supported. Objects in the scene hierarchy can be dragged and dropped onto another object, in order to create a parent-child relationship. The scene hierarchy will display a different content if the simulator is in an edit-mode state. Refer to the  shape edit modes  for more information. \n \n \n \n [Scene hierarchy] \n \n \n page: each  scene  may contain up to 8 pages, each of them may contain an unlimited number of views. A page can be seen as container for views. Refer to the  pages and views section  for more details. \n views: there can be an unlimited number of views contained in a page. A view is used to display the scene (itself containing an environment and objects), seen through a  viewable object  (e.g.  cameras ,  graphs  or  vision sensors ). \n  information text: the information text displays information related to current object/item selection and to running simulation states or parameters. The text display can be toggled with one of the two small buttons on the upper left side of a page. The other button can be used to toggle a white background, giving a better contrast depending on the background color of a scene. \n  status bar: the status bar displays information related to performed operations, commands, and also displays error messages from script interpreters. From within a  script  the user can also output strings to the status bar or console with the  sim.addLog  function. The status bar displays only two lines by default, but it can be resized using its horizontal separation handle. \n  Lua commander: a read-eval-print loop, that adds a text input to the CoppeliaSim status bar, allowing to enter and execute Lua code on the fly, like in a terminal. The code can be run in  the sandbox script , or any other active script in CoppeliaSim. \n custom user interfaces:  custom user interfaces  are user-defined UI surfaces that can be used to display information (text, images, etc.) or a custom dialog, allowing to interact with the user in a customized way. \n popup menu: popup menus are the menus that appear after a right mouse button click. To activate a popup menu, make sure the mouse doesn\'t move during the click operation, otherwise the camera rotation mode may be activated (see the  camera section  for more details). Each surface within the application window (e.g. scene hierarchy view, page, view, etc.) may trigger a different popup menu (context-sensitive). The content of popup menus may also change depending on the current simulation state or edit mode. Most popup menu function can also be accessed through the menu bar, except for the view-menu item that only appears when the popup menu is activated on a view or page. \n \n', 'tags': '', 'url': 'Interface.html'}, {'title': 'Model dialog', 'text': 'Model dialog \n The properties of a model can be individually adjusted in the model dialog. It can be opened with a double-click on a  model icon (注意與  object icon  的差別) in the  scene hierarchy : \n \n [Model icon in the scene hierarchy] \n \n \n [Model dialog] \n \n \n Select model thumbnail: when saving a model, a dialog pops open asking for a model thumbnail (that will be displayed in the  model browser ). If however you wish to save a thumbnail of your model in a different configuration (e.g. you wish to save a model of a snake robot in straight configuration, but you want the thumbnail to visualize the snake robot in a bent configuration), then you can specify the thumbnail here. \n Override properties: here you can disable (override) specific properties for the whole model (i.e. for all objects in the model hierarchy tree). This is convenient to quickly disable a model that takes too much calculation time for instance. See also the sections on  collidable objects ,  measurable objects  and  detectable objects , and the  sim.setModelProperty API function . \n Model content acknowledgments/Info: information related to a model. It is always good practice to acknowledge the original author of a model, or imported mesh. When a model that contains acknowledgment information is opened, it will automatically display that information. \n \n', 'tags': '', 'url': 'Model dialog.html'}, {'title': 'Collidable', 'text': 'For bubbleRob: \n to inform CoppeliaSim that both objects do not mutually collide, and we do this in following way: in the  shape dynamics properties , for slider we set the local respondable mask to 00001111, and for bubbleRob, we set the local respondable mask to 11110000. \n Collidable objects \n Collidable objects are  objects  that can be tested for  collision  against other collidable objects, i.e. that will register a collision state. This does not mean that they will respond to collision (i.e.  respondable ), which is something different. Collidable objects include: \n \n \n Dummies \n Shapes \n OC trees \n Point clouds \n \n \n Dummies and point clouds, since point-based, can only collide against OC trees (which are volume-based). \n Collections  are also collidable, since they might contain collidable objects. \n Collidable objects can have their  collidable property  individually enabled or disabled (enabled by default for non-pure shapes, OC trees and point clouds). This can be set in the  object common properties  or through the  sim.setObjectSpecialProperty   API  function. \n Additionally, collidable objects can have their  collidable property  overridden depending on their related  model properties  (if they are part of a  model ). Refer to the  model dialog  for more information.', 'tags': '', 'url': 'Collidable.html'}, {'title': 'Measurable', 'text': 'Measurable objects \n Measurable objects are  objects  that can be used for  minimum distance calculation  against other measurable objects. They include: \n \n \n Dummies \n Shapes \n OC trees \n Point clouds \n \n \n Collections  are also measurable, since they might contain measurable objects. \n Measurable objects can have their  measurable property  individually enabled or disabled (enabled by default for non-pure shapes, OC trees and point clouds). This can be set in the  object common properties  or through the  sim.setObjectSpecialProperty   API  function. \n Additionally, measurable objects can have their  measurable property  overridden depending on their related  model properties  (if they are part of a  model ). Refer to the  model dialog  for more information. \n', 'tags': '', 'url': 'Measurable.html'}, {'title': 'Detectable', 'text': 'Detectable objects \n Detectable objects are  objects  that can be detected by  proximity sensors . They include: \n \n \n Dummies \n Shapes \n OC trees \n Point clouds \n \n \n Dummies and point clouds, since point-based, can not be detected by ray-type or randomized-type proximity sensors. \n Detectable objects can be detected by all proximity sensors, or only by specific types of proximity sensors, or a sub-category of proximity sensors as listed: \n \n \n Ultrasonic proximity sensors \n Infrared proximity sensors \n Laser proximity sensors \n Inductive proximity sensors \n Capacitive proximity sensors \n \n \n Collections  are also detectable, since they might contain detectable objects. \n Detectable objects can have their  detectable property  individually enabled or disabled, and this for all types of proximity sensors (enabled by default for non-pure shapes). This can be set in the  object common properties  or through the  sim.setObjectSpecialProperty   API  function. \n Additionally, detectable objects can have their  detectable property  overridden depending on their related  model properties  (if they are part of a  model ). Refer to the  model dialog  for more information. \n', 'tags': '', 'url': 'Detectable.html'}, {'title': 'Shape', 'text': 'Shape properties \n The shape properties are part of the  scene object properties  dialog, which is located at [Menu bar --> Tools --> Scene object properties]. You can also open the dialog with a double-click on an object icon in the  scene hierarchy , or with a click on its  toolbar  button: \n \n [Scene object properties toolbar button] \n \n In the scene object properties dialog, click the Shape button to display the shape dialog (the Shape button only appears if the last selection is a  shape ). The dialog displays the settings and parameters of the last selected shape. If more than one shape is selected, then some parameters can be copied from the last selected shape to the other selected shapes (Apply to selection-buttons): \n \n [Shape dialog] \n \n \n Adjust color: allows editing the colors of the shape. \n Shading angle: the shading angle is the angle from which individual faces are distinguished. This only affects the visual appearance of a shape. A small angle makes a shape appear sharp, with many edges, a large angle makes a shape appear smooth and with less edges. \n Show edges: displays edges in black. \n Backface culling: each triangle composing a shape has an inside and an outside face. When Backface culling is enabled, then inside faces won\'t be displayed. This is a useful parameter for closed shapes and for shapes that are transparent.  \n Invert faces: this flips all triangles. Inside faces become outside faces and vice-versa. Convex shapes will become non-convex, except for pure shapes. \n Adjust texture: opens the  texture dialog  for the selected shape. When a shape is associated with a texture, it will be displayed in a textured way. \n Quick textures (selection): applies a cubic mapped texture to all selected shapes. This is specially useful with seamless textures used as "dirt", in order to make objects look more realistic. \n Clear textures (selection): removes the texture from all selected shapes. \n View/modify geometry: opens the  shape geometry dialog  for the selected shape. It allows to adjust various parameters of the mesh. \n Show dynamic properties dialog: toggles the  shape dynamics properties  dialog. The shape dynamics dialog allows to adjust a shape\'s dynamics properties. \n \n \n Some of above\'s parameters are only available for simple shapes. When a compound shape is selected, then you can edit its visual attributes by switching to the shape edit mode for compound shapes. You can of course also ungroup it in order to individually edit its components. \n', 'tags': '', 'url': 'Shape.html'}, {'title': 'Position', 'text': "Position dialog \n The position dialog becomes visible when the object translation toolbar button is selected: \n \n [Object translation toolbar button] \n The dialog has four distinct tabs: \n Mouse Translation \n \n [Mouse translation tab] \n \n In this section of the dialog, translation parameters of objects manipulated with the mouse can be set. See also the page on  object movement via the mouse . \n \n \n Relative to world/own frame: indicates that a mouse drag will translate the selected object on a plane or line that is aligned with the absolute reference frame, or aligned with the object's own reference frame. \n Translation step size: the step size used when translating the selected object with a mouse drag (see the  user settings dialog  for default step sizes). Smaller step sized can still be used during manipulation by pressing the shift-key after the mouse button was pressed down. \n Preferred axes: along X/ along Y/ along Z: indicates that a mouse drag allows translating the selected object along preferred axes of the reference frame selected above. Other axes can be used during manipulation by pressing the ctrl-key after the mouse button was pressed down. \n \n \n \n Position \n \n [Position tab] \n \n In this section of the dialog, precise positioning can be achieved on objects or items. \n \n \n Relative to world/parent frame: indicates that the coordinates are relative to the absolute reference frame, or relative to the parent reference frame. \n X-/ Y- / Z-coordinate: position of the selected object relative to the indicated reference frame (world or parent). \n \n \n \n Translation \n \n [Translation tab] \n \n In this section of the dialog, precise object or item translation can be achieved. \n \n \n Relative to world/parent frame/own frame: indicates that the translation will be relative to the absolute reference frame, relative to the parent reference frame, or relative to the object's own reference frame. \n Translate along X / Y / Z: indicates the desired translation amounts along the x-, y- and z-axis of the indicated reference frame (world, parent or own frame). \n \n \n \n Position scaling \n \n [Position scaling tab] \n \n In this section of the dialog, precise scaling of object or item's position can be achieved. \n \n \n Relative to world/parent frame: indicates that the position scaling will be relative to the absolute reference frame, or relative to the parent reference frame. \n Scale along X / Y / Z: indicates the desired position scaling along the x-, y- and z-axis of the indicated reference frame (world or parent). \n \n", 'tags': '', 'url': 'Position.html'}, {'title': 'Orientation', 'text': "Orientation dialog \n The orientation dialog becomes visible when the object rotation toolbar button is selected: \n \n [Object rotation toolbar button] \n The dialog has three distinct tabs: \n Mouse Rotation \n \n [Mouse rotation tab] \n \n In this section of the dialog, rotation parameters of objects manipulated with the mouse can be set. See also the page on  object movement via the mouse . \n \n \n Relative to world/own frame: indicates that a mouse drag will rotate the selected object about an axis of the absolute reference frame, or the object's own reference frame. \n Rotation step size: the step size used when rotating the selected object with a mouse drag (see the  user settings dialog  for default step sizes). Smaller step sized can still be used during manipulation by pressing the shift-key after the mouse button was pressed down. \n Preferred axis: about X/ about Y/ about Z: indicates that a mouse drag allows rotating the selected object about a preferred axis of the reference frame selected above. Other axes can be used during manipulation by pressing the ctrl-key after the mouse button was pressed down. \n \n   \n \n Orientation \n \n [Orientation tab] \n \n In this section of the dialog, setting a precise object orientation can be achieved. \n \n \n Relative to world/parent frame: indicates that the indicated  Euler angles  are relative to the absolute reference frame, or relative to the parent reference frame. \n Alpha / Beta / Gamma:  Euler angles  of the selected object relative to the indicated reference frame (world or parent). \n \n   \n \n Rotation \n \n [Rotation tab] \n \n In this section of the dialog, precise object rotation can be achieved. \n \n \n Relative to world/parent frame/own frame: indicates that the rotation will be relative to the absolute reference frame, relative to the parent reference frame, or relative to the object's own reference frame. \n Rotate around X / Y / Z: indicates the desired rotation amounts around the x-, y- and z-axis of the indicated reference frame (world, parent or own frame). \n \n \n", 'tags': '', 'url': 'Orientation.html'}, {'title': 'Sensors', 'text': 'Proximity sensors  \n CoppeliaSim offers a very powerful and efficient way to simulate proximity sensors. The user can model almost any type of proximity sensor, from ultrasonic to infrared, and so on. The  scene objects  that allow for this functionality are proximity sensors (which are different from  vision sensors ), which can detect  detectable   entities . Following figures illustrates  simulations  using proximity sensors: \n \n [Mobile robots using proximity sensors] \n \n Proximity sensors are added to the  scene  with [menu bar --> Add --> Proximity sensor]. \n The proximity sensor detection routines used by the proximity sensors are also available as stand-alone routines via the  Coppelia geometric routines . \n', 'tags': '', 'url': 'Sensors.html'}, {'title': 'Sensor volume', 'text': 'Proximity sensor volume dialog \n The proximity sensor volume dialog is part of the  proximity sensor properties . The dialog displays the volume settings and parameters of the last selected  proximity sensor . If no object is selected, the dialog is inactive. If more than one proximity sensor is selected, then some parameters can be copied from the last selected proximity sensor to the other selected proximity sensors (Apply to selection-buttons): \n \n [Proximity sensor volume dialog] \n \n The six types of the volume of a proximity sensor (ray, randomized ray, pyramid, cylinder, disc and cone) are defined via several parameters: \n \n [Ray, pyramid and cylinder-type detection volume parameters] \n \n \n [Disk and cone-type detection volume parameters] \n \n \n Face count, face count far, subdivisions and subdivisions far can be used to describe the detection volume with more or less precision. More precision also implies longer calculation times. \n Inside gap: parameter only available for the disc- and cone-type proximity sensors. Its value can vary between 0 and 1 and represents the amount of volume (proportionally) to remove. This feature can be used to create very sophisticated detection volumes by combining several sensors: \n \n \n \n [Inside gap functionality and its application] \n', 'tags': '', 'url': 'Sensor volume.html'}, {'title': 'Sensor parameters', 'text': 'Proximity sensor detection parameter dialog \n The proximity sensor detection parameter dialog is part of the  proximity sensor properties . The dialog displays various detection parameters of the last selected  proximity sensor . \n \n [Proximity sensor detection parameter dialog] \n \n \n Front / back face detection: the user can decide which side of a triangle the sensor will  see  and detect. Front faces appear as blue, back faces appear as red in the  triangle edit mode . \n Fast detection (approximate): when selected, the detection process will be faster, but not exact anymore (i.e. the detected point might not be the closest point). \n Limited angle detection: when selected, a maximum angle between the detection ray and an object\'s face normal vector can be specified. This is a useful feature when modeling ultrasonic proximity sensors. Ultrasonic proximity sensors normally cannot  see  surfaces that don\'t face the sensor enough. When enabled and the maximum angle is small, calculation time can be drastically increased. \n \n \n \n [Limited angle detection principle. Yellow arrows indicate surfaces normal vectors. Maximum angle is 30° (for example)] \n \n \n [Two identical proximity sensors configurations. Only one proximity sensor has the limited angle detection turned on (right)] \n \n \n Don\'t allow detections if distance smaller than: some sensors have a minimum detection distance (e.g. ultrasonic sensors) from which the sensor won\'t be operating anymore (i.e. the object to be detected is too close from the sensor). To model this behavior, one could simply set an offset for the detection volume, but by doing so the sensor will still be able to detect other objects that are located farther away. A real sensor would have its "view field" blocked by the closer object and not detect anything. This can be modeled by indicating a minimum detection distance (a mask distance) that, if undershot, would simply disable detection. \n \n \n \n [Two proximity sensors, one without mask distance (left), the other with mask distance (right)] \n \n \n Randomized ray detection: randomized ray detection is available only for randomized ray type proximity sensors. During randomized ray detection, a ray will sweep a cone-shaped volume in a random fashion. You can specify how many random rays the sensor should check (ray count), and how many ray detections are required to trigger the sensor (ray detections count for triggering). \n \n', 'tags': '', 'url': 'Sensor parameters.html'}, {'title': 'Joints', 'text': "Joints  \n A joint is an  object  that allows for a relative movement between its parent and its child/children. When a parent-child relationship is built between a joint and an object, the object is attached to the joint's second reference frame, thus, a change of the joint's linear/angular position will directly be reflected onto its children. Joints can operate in different  modes . New joints can be added to a  scene  with [Menu bar --> Add --> Joints]. \n \n [Robotic manipulator with highlighted joints] \n \n 4 types of joints are supported: \n \n [Revolute joint, prismatic joint, screw and spherical joint] \n \n \n Revolute joint: a revolute joint has one DoF and is used to describe a rotational movement between objects. The rotation happens about its reference frame's z-axis \n Prismatic joint: a prismatic joint has one DoF and is used to describe a translational movement between objects. The translation happens along its reference frame's z-axis \n Screw: a screw, which can be seen as a combination of a revolute and prismatic joint with linked values, has one DoF and is used to describe a movement similar to a screw. A pitch parameter defines the amount of translation for a given amount of rotation. The translation/rotation happens along/about its reference frame's z-axis \n Spherical joint: a spherical joint has three DoFs and is used to describe a rotational movement between objects. In some situations, a spherical joint can be thought of as 3 concurrent and orthogonal joints, that are parented in a hierarchy-chain. The analogy is however only valid while all revolute joints keep an orientation distinct from any of the other two: indeed, should two joints come close to coincide, a singular situation might appear and the mechanism will lose one DoF. This does never happen with spherical joints, which are internally handled to avoid this kind of situation: \n \n \n \n [Two equivalent mechanisms (in this configuration): spherical joint (left) and 3 revolute joints (right)] \n \n \n [Two non-equivalent mechanisms: the right configuration is close to a singularity] \n", 'tags': '', 'url': 'Joints.html'}, {'title': 'Joint properties', 'text': 'Joint dynamics properties \n The joint dynamics properties are part of the  joint properties . Its dialog displays the dynamics settings and parameters of the last selected  joint . If no joint is selected, or the joint is not dynamic, then the dialog is inactive. If more than one joint is selected, then some parameters can be copied from the last selected joint to the other selected joints: \n \n [Joint dynamics dialog] \n \n Several control modes are supported: \n \n \n Free (no control): the joint is free, i.e. non-motorized \n Force/torque control: the joint is controlled in force/torque, i.e. a constant force/torque is applied. Can be modulated with  sim.setJointTargetForce \n Velocity control: the joint is controlled in velocity: the specified force/torque is applied until the desired velocity is reached. Optionally, a specific motion profile for the velocity can be applied. Can be modulated with  sim.setJointTargetVelocity  and  sim.setJointTargetForce \n Position control: the joint is controlled in position: the specified force/torque is applied and the velocity is modulated via a PID controller, or via specific motion constraints, until the target position/angle is reached. Parameters can be adjusted with  sim.setJointTargetPosition ,  sim.setJointTargetForce  and  sim.setObjectFloatParam \n Spring-damper control: the joint is controlled in position by modulating the exerted force/torque via a simple KC controller, trying to reach the zero displacement position/angle. Parameters can be adjusted with  sim.setJointTargetPosition ,  sim.setJointTargetForce  and  sim.setObjectFloatParam \n Custom control: the joint calls a  joint callback function  for control, where the user can decide of a force/torque and target velocity in a flexible way \n \n', 'tags': '', 'url': 'Joint properties.html'}, {'title': 'Force sensor', 'text': 'Force sensors  \n Force sensors are initially rigid links between two  shapes  that are able to measure transmitted forces and torques. The rigidity of force sensors is conditional, in the sense that force sensors can be  broken  if a certain condition arises (e.g. if a force or torque threshold is overshot). Following figure illustrates an application using a force sensor: \n \n [Force sensor object (green) measuring the force and torque exerted by a beam (blue) anchored in a wall (red)] \n \n A force sensor measures a pair of 3 values representing the force on the sensor along the x-, y- and z-axis, and the torque on the sensor about the x-, y- and z-axis: \n \n [Forces and torques measured by a force sensor] \n \n Initially, a force sensor acts as a rigid link. During  simulation , a force sensor can however be  broken  when a specified force/torque threshold is overshot or when some other user-defined conditions are met. Following figures illustrate the  broken  state of a force sensor: \n \n [Broken force sensor] \n \n A force sensor is only operational during simulation if it is dynamically enabled. For more information on dynamically enabled force sensors, refer also to the section on  designing dynamic simulations .  Joints  are also able to measure a force or a torque, however only along/about their z-axis. \n Force sensors are added to the  scene  with [Menu bar --> Add --> Force sensor]. \n', 'tags': '', 'url': 'Force sensor.html'}, {'title': 'Dynamic simulation', 'text': 'Designing dynamic simulations \n In CoppeliaSim, only a limited number of  objects  will be dynamically simulated. Those are  shapes ,  joints  and  force sensors , but it will depend on the  scene  structure and object properties, whether a given object will be dynamically simulated. Dynamically simulated objects can be easily recognized during  simulation , since following icon will appear next to the object\'s alias in the  scene hierarchy : \n \n [Icon marking dynamically simulated objects] \n \n Double-clicking the icon in the scene hierarchy (during simulation only) will display some information related to the object\'s dynamic behavior. Objects that are supposed to by dynamically simulated but which, for a reason or another cannot be dynamically simulated, will display following icon instead: \n \n [Warning icon when an object cannot be dynamically simulated] \n \n Static/non-static, respondable/non-respondable shapes \n Shapes  can be classified into 4 types depending on their behavior during dynamic simulation: \n \n [Dynamic simulation main shape types] \n \n During dynamic simulation, static shapes will not be influenced (i.e. their position relative to their parent object is fixed), whereas non-static shapes will be directly influenced by gravity or other constraints (e.g. dynamically enabled joints, see hereafter). Respondable shapes influence each other during dynamic collision (i.e. they produce a mutual  collision reaction , they will bounce off each other). Following figure illustrates the static/non-static, respondable/non-respondable behaviors: \n \n [Static/non-static, respondable/non-respondable shape behaviors and interactions] \n \n Two respondable shapes will always produce a collision reaction, unless their respective collision masks don\'t overlap. Static/non-static, respondable/non-respondable shape properties, as well as collision masks can be set in the  shape dynamics properties  dialog. \n \n Simple constraints between shapes or force sensors \n Non-static shapes will fall (i.e. be influenced by gravity) if they are not otherwise constrained. Dynamic constraints between shapes can be set-up by attaching two shapes together via a  dynamically enabled   joint , a  dynamically enabled   force sensor , or via a  dummy -dummy  dynamic overlap constraint . \n Following represent a simple constraint between two shapes: \n \n [Dynamically enabled joint or force sensor] \n \n Above joint is considered to be dynamically enabled. \n \n Loop closures and other constraints \n Sometimes a mechanism displays loops, e.g. in parallel machines, and other times additional rigid constraints between shapes are required. In those situations, special dummy-dummy  dynamic overlap constraints  can be introduced: \n \n [Dynamic overlap constraint involving a joint or force sensor] \n \n Above joint is also considered to be dynamically enabled. \n Following represents a rigid constraint between two shapes, that do not have to be in the same hierarchy tree/branch: \n \n \n [Dynamic overlap constraint, that rigidly binds two shapes] \n \n Rigid compounds \n Two non-static  shapes  that are not linked through dynamically enabled joints or force sensors will move independently from each other during dynamic simulation. If you want two or more shapes to behave as one single shape, you will have to group them (Menu Bar --> Edit --> Grouping/Merging --> Group selected shapes). Make sure you adjust the moment of inertia of the final shape appropriately. Make sure you also read the  section about pure shapes  further down: \n \n [Compound of several non-static shapes to obtain a single non-static shape] \n \n Compound shapes will behave as a rigid entity and also share identical dynamic properties (e.g. friction coefficient). Sometimes a rigid entity with differentiated dynamic properties is needed. In that case, shapes can be rigidly linked via a force sensor. \n \n Design consideration 1 \n Use pure shapes. Whenever possible, try using  pure shapes  as respondable shapes: pure shapes are much more stable and faster during dynamic simulation. Instead of using the complicated triangular mesh of a robot model as respondable shape, or its slightly better convex representation, try approximating the triangular mesh with several pure shapes, then grouping them [Menu bar --> Edit --> Grouping/Merging --> Group selected shapes] (beware that if you merge pure shapes instead of grouping them, the resulting shape will not be a pure shape anymore). Following figure illustrates a robot\'s hidden respondable pure shapes: \n \n [Dynamic robot model (left) and underlying pure non-static shapes used for dynamic simulation (right)] \n \n A convenient way to extract pure shapes from complex non-pure shapes, is to enter the  triangle edit mode  for the complex shape, then selecting regions of interest and extracting rectangular, spherical or cylindrical pure shapes. Refer to the Extract cuboid, Extract cylinder, and Extract sphere buttons in the triangle edit mode. It is also always good practice to visualize and verify a scene\'s dynamic content using the dynamic content visualization toolbar button (see the  design consideration 3 ). \n When a body can collide, but not constantly, or does not play a important role in the stability of a mechanism/robot, then it is not absolutely necessary to use pure shapes, and convex shapes could be a viable alternative too: \n \n Design consideration 2 \n Use convex shapes instead of random shapes. Whenever possible, try using  pure shapes  as respondable shapes (see  design consideration 1 ). This is however not always easy to do, or practical (think of a torus for instance). In that case, you can generate a convex shape, or a convex decomposed shape (i.e. a simple/compound shape containing only convex meshes). Convex shapes perform faster and are more stable than random shapes (but they are still not as fast and stable as pure shapes!). \n Select the shapes you wish to simplify as convex shapes, then select [Menu bar --> Edit --> Morph selected shapes into convex shapes...]. Refer also to the item [Menu bar --> Add --> Convex decomposition of selected shapes...]. Following figure illustrates a convex decomposition: \n \n [Non-convex model (left) and corresponding convex-decomposed model (right)] \n \n Design consideration 3 \n Carefully inspect the dynamic content of a scene. It can sometimes be a little bit confusing to work with hidden shapes meant to play an active role in the simulation. This is often the case with dynamically enabled shapes, joints or force sensors: indeed, they are most of the time hidden to the viewer\'s eye. The dynamic content can however always be inspected DURING a simulation, by activating the dynamic content visualization button: \n \n [Dynamic content visualization button] \n \n \n [Normal and "dynamic content" scene visualization] \n \n Dynamic objects will appear in various colors, depending on their function or settings. Refer to following figure: \n \n [Color coding for dynamic objects (in dynamic content visualization mode)] \n \n Notice in above\'s figure the appearance of non-pure shapes: they have their triangular mesh contour represented in green or grey color. Dynamically simulated non-pure shapes should be avoided at all cost, since they require much longer calculation times, and present a not very stable behaviour (convex shapes are however faster and more stable than non-convex shapes). Refer also to the  design consideration 1  here above. \n \n Design consideration 4 \n Use a simple hierarchical structure. When building a model that is meant to be simulated dynamically, attach all non-dynamic objects to the dynamic objects (non-static shapes and dynamically enabled joints). Above\'s wheeled robot model is illustrated in following figure: \n \n [Dynamic elements in a robot model] \n \n Design consideration 5 \n Carefully chose the model base object. When building a dynamic  model , actually also when building a static model, always carefulls consider what role the model will be playing. Will it be used on its own? Can it be built on top of another model or object? Or can it accept other models or objects to be built on top of it? Consider following example: \n You have created a model of a mobile robot. You have also created a model of a gripper. Clearly you want to be able to easily attach the gripper model on top of the robot model. The other way round will never make sense (attaching the robot model on top of the gripper model). You might also want to use the robot model and the gripper model on their own. Following is the view of the  scene hierarchy  of a possible model definition: \n \n [Example model definition of a robot and a gripper] \n \n Notice the model icon next to the \'robot\' and \'gripper\' shape objects. This indicates that all objects built on top of them are part of the same model definition. The two models operate well on their own. However if you try to attach the gripper on top of the robot (by selecting the gripper model, then the robot model, then clicking [Menu bar --> Edit --> Make last selected object parent]), the gripper will not be staying fixed on the robot during simulation. Following is the scene hierarchy of above two models, where the gripper has been attached to the robot: \n \n [Non-functional robot-gripper model assembly] \n \n Notice in above scene hierarchy how the  pure shape  "gripper" is built on top of the pure shape "robot". And remember that non-static shapes will fall if not otherwise constrained by a  joint  of  force sensor ... exactly, we need a force sensor in-between "gripper" and "robot" in order to have a rigid connection between both! \n The correct model definition of the robot would have to include an attachment point (or several of them) for the gripper as illustrated in following figure: \n \n [Example model definition of a robot with an attachment point] \n \n The attachment point is a simple force sensor. Assembling the gripper model with the robot model above results in a gripper that stays fixed relative to the robot: \n \n [Functional robot-gripper model assembly] \n \n To simplify the assembly procedure even more, you can customize the behaviour of the  assembling  toolbar button , in order to automatically put the gripper onto the attachment point, with the correct relative position/orientation. For further details refer to the  section on models  and the dialog item Assembling in the  object common properties . \n \n Design consideration 6 \n Use reasonable sizes. Shapes that are very long and thin, or that are too small might behave strangely (jittering, jumping). Try keeping sizes above 3 centimeters if possible. Otherwise you can adjust the internal scaling parameter in the  dynamics engines general properties  dialog. \n This requirement can be loosened by using a different engine, or by selecting a more precise  dynamics configuration mode . \n \n Design consideration 7 \n Keep masses similar and not too light. When linking two shapes with a dynamically enabled joint or a dynamically enabled force sensor, make sure the two shape\'s masses are not too different (m1<10*m2 and m2<10*m1), otherwise the joint or force sensor might be very  soft  and  wobbly  and present large positional/orientational errors (this effect can however also be used as a  natural damping  sometimes). Additionally, very low mass shapes should be avoided since they won\'t be able to exert very large forces onto other shapes (even if propelled by high force actuators!). \n This requirement can be loosened by using a different engine, or by selecting a more precise  dynamics configuration mode . \n \n Design consideration 8 \n Keep principal moments of inertia* relatively large. Try keeping the  principal moments of inertia / mass  (*refer to the  shape dynamics properties  dialog) relatively large, otherwise mechanical chains might be difficult to control and/or might behave in a strange way. \n This requirement can be loosened by using a different engine, or by selecting a more precise  dynamics configuration mode . \n \n Design consideration 9 \n Assign dynamic shapes to layer 9. Assign all dynamic shapes which are supposed to be hidden to layer 9 (refer to the  object common properties ): in CoppeliaSim, by default, all layers are visible, except for layers 9-16. When editing or testing a  scene , you can then quickly visualize all hidden shapes in the scene by temporarily enabling layer 9 (refer also to the  layer selection dialog ). \n \n Design consideration 10 \n Never have a static shape between two dynamic items. The static shape will interrupt the logical behaviour of the dynamic chain: \n \n [Wrong and correct construction] \n \n Design consideration 11 \n Never have a static respondable shape on top of a dynamic item. Static means the shape\'s trajectory cannot be influenced by any collision, but if at the same time it is respondable, this means that it can itself influence other shapes\' trajectories via collision. The simulation result would be unpredictable. \n', 'tags': '', 'url': 'Dynamic simulation.html'}, {'title': 'Scripts', 'text': "Scripts \n lua \n CoppeliaSim is a highly customizable simulator: almost every step of a  simulation  is user-defined. Additionally, the simulator itself can also be customized to a large extent. This flexibility is allowed through an integrated script interpreter ( Lua ) and an external script interpreter ( Python ). Both operate in almost the same fashion,  except for small differences . For more information on Lua, refer to the  Lua crash course  section and the  online documentation . \n \n [Lua and Python logo] \n \n CoppeliaSim extends Lua's and Python's API function and adds CoppeliaSim specific commands that can be recognized by their  sim -prefixes (e.g.  sim.getObjectPosition ). For a list of all CoppeliaSim specific API functions, refer to the  regular API . New, customized API functions can also be registered from a  plugin . Refer to the  related API-functions  for more information. \n Several types of scripts are supported in CoppeliaSim:  the sandbox script ,  add-ons , and  embedded scripts  (themselves composed of a  main script ,  child scripts  and  customization scripts ). An embedded script is a script that is embedded in a  scene  (or  model ), i.e. a script that is part of the scene and that will be saved and loaded together with the rest of the scene (or model). On the other hand, the sandbox script and add-ons are not associated with any specific scene, model or object, and will run across all scenes in a similar way. \n \n [Script types] \n \n Scripts are invoked via  callback functions  by CoppeliaSim, and follow a  specific execution order . They can  run threaded or non-threaded . \n CoppeliaSim scripts may be published under any license. \n", 'tags': '', 'url': 'Scripts.html'}, {'title': 'bubbleRob scripts', 'text': 'non threaded lua and python for bubbleRob with version control: \n 2023_cd_bubbleRob_lua_py.7z \n for Python script to be executed under CoppeliaSim: \n system/usrset.txt \n defaultPython = Y:/Python311/python.exe executeUnsafe = true \n pip install pyzmq cbor \n Lua local variables and blocks:  https://www.lua.org/pil/4.2.html \n function sysCall_init()\n         \n    -- This is executed exactly once, the first time this script is executed\n    bubbleRobBase=sim.getObject(\'.\') -- this is bubbleRob\'s handle\n    leftMotor=sim.getObject("./leftMotor") -- Handle of the left motor\n    rightMotor=sim.getObject("./rightMotor") -- Handle of the right motor\n    noseSensor=sim.getObject("./sensingNose") -- Handle of the proximity sensor\n    minMaxSpeed={50*math.pi/180,300*math.pi/180} -- Min and max speeds for each motor\n    backUntilTime=-1 -- Tells whether bubbleRob is in forward or backward mode\n    robotCollection=sim.createCollection(0)\n    sim.addItemToCollection(robotCollection,sim.handle_tree,bubbleRobBase,0)\n    distanceSegment=sim.addDrawingObject(sim.drawing_lines,4,0,-1,1,{0,1,0})\n    robotTrace=sim.addDrawingObject(sim.drawing_linestrip+sim.drawing_cyclic,2,0,-1,200,{1,1,0},nil,nil,{1,1,0})\n    graph=sim.getObject(\'./graph\')\n    distStream=sim.addGraphStream(graph,\'bubbleRob clearance\',\'m\',0,{1,0,0})\n    -- Create the custom UI:\n    xml = \'<ui title="\'..sim.getObjectAlias(bubbleRobBase,1)..\' speed" closeable="false" resizeable="false" activate="false">\'..[[\n                <hslider minimum="0" maximum="100" on-change="speedChange_callback" id="1"/>\n            <label text="" style="* {margin-left: 300px;}"/>\n        </ui>\n        ]]\n    ui=simUI.create(xml)\n    speed=(minMaxSpeed[1]+minMaxSpeed[2])*0.5\n    simUI.setSliderValue(ui,1,100*(speed-minMaxSpeed[1])/(minMaxSpeed[2]-minMaxSpeed[1]))\n    \nend\n\nfunction sysCall_sensing()\n    local result,distData=sim.checkDistance(robotCollection,sim.handle_all)\n    if result>0 then\n        sim.addDrawingObjectItem(distanceSegment,nil)\n        sim.addDrawingObjectItem(distanceSegment,distData)\n        sim.setGraphStreamValue(graph,distStream,distData[7])\n    end\n    local p=sim.getObjectPosition(bubbleRobBase,sim.handle_world)\n    sim.addDrawingObjectItem(robotTrace,p)\nend \n\nfunction speedChange_callback(ui,id,newVal)\n    speed=minMaxSpeed[1]+(minMaxSpeed[2]-minMaxSpeed[1])*newVal/100\nend\n\nfunction sysCall_actuation() \n    result=sim.readProximitySensor(noseSensor) -- Read the proximity sensor\n    -- If we detected something, we set the backward mode:\n    if (result>0) then backUntilTime=sim.getSimulationTime()+4 end \n\n    if (backUntilTime<sim.getSimulationTime()) then\n        -- When in forward mode, we simply move forward at the desired speed\n        sim.setJointTargetVelocity(leftMotor,speed)\n        sim.setJointTargetVelocity(rightMotor,speed)\n    else\n        -- When in backward mode, we simply backup in a curve at reduced speed\n        sim.setJointTargetVelocity(leftMotor,-speed/2)\n        sim.setJointTargetVelocity(rightMotor,-speed/8)\n    end\nend\n\nfunction sysCall_cleanup() \n    simUI.destroy(ui)\nend \n', 'tags': '', 'url': 'bubbleRob scripts.html'}, {'title': 'Leo Editor', 'text': "recursive importer: \n '''Recursively import all python files in a directory and clean the result.'''\n# ctrl + b to execute\n\nc.recursiveImport(\n    dir_ = r'./',\n    kind = '@clean', # The new best practice.\n    safe_at_file = False,\n    theTypes = ['.lua'] \n) \n get_xml_tags.py \n # from https://stackoverflow.com/questions/29596584/getting-a-list-of-xml-tags-in-file-using-xml-etree-elementtree\nimport xml.etree.ElementTree as ET\n# load and parse the file\nxmlTree = ET.parse('two_links_exhaustive.simscene.xml')\n\nelemList = []\n\nfor elem in xmlTree.iter():\n    elemList.append(elem.tag)\n\n# now I remove duplicities - by convertion to set and back to list\nelemList = list(set(elemList))\n\n# Just printing out the result\nprint(len(elemList))\n\nfor i in elemList:\n    print(i) \n @data import_xml_tags \n coppeliasim_xml_tags.7z", 'tags': '', 'url': 'Leo Editor.html'}]};